\section{\xxx Background}\label{sec:background}

This section introduces the background of two key techniques in \xxx, the 
\paxos consensus protocol (\S\ref{sec:paxos}) and RDMA features 
(\S\ref{sec:rdma}).

\subsection{\paxos}\label{sec:paxos}
% \paxos background. Keys:
% Leader, backup.
% Persistent storage. 
% Network round trips in normal case. Latency.
An SMR system runs the same program and its data on a set of machines 
(replicas), and it uses a distributed consensus protocol (typically, \paxos) 
to coordinates inputs across replicas. For efficiency, in normal case, \paxos 
often let one replica work as the leader which invoke consensus requests, and 
the other replicas work as backups to agree on or reject these requests. If 
the leader fails, \paxos elects a new leader.

When a new input comes, \paxos starts a new consensus round, which invokes a 
consensus request on this input to the other replicas. \paxos guarantees that 
all replicas consistently agree to process this input as long as a majority of 
replicas' agreement. This quorum based consensus makes \paxos tolerate various 
faults such as machine failures and network crashes. Before a replica agrees on 
a input, \paxos logs this input in the replica's persistent storage for 
durability. As rounds move on, \paxos consistently enforce the same sequence of 
inputs among replicas. If a program runs as a deterministic state machine (\ie, 
given the same input, the program always produces the same output), \paxos 
guarantees that programs on active replicas never diverge.

% Normal case, round trip.
Netork latency of consensus messages is one key challenge to make SMR support 
general server programs which demand high perforrmance, espeically in-memory 
storage servers. Because each input input is processed by a server program, 
existing consensus protocols involve consensus messages on TCP or UDP, which go 
through software network layers and the OS kernel, causing hundreds of \us 
latency in LAN. For instance, even in an efficient \paxos protocol, each input 
in normal case takes two consensus messages between every two replicas (one a 
request from leader to backup and the other a reply from backup to leader).


\subsection{RDMA}\label{sec:rdma}
% RDMA background.
% Cheap, pervasive.
% One-sided write operations. Faster than IPoIB.
RDMA recently has become commonplace in Datacenter networking due to its high 
performance and its decreashing price. For instance, a machine with 40Gbps RDMA 
NIC and 24-cores costs 3.8K US \$, and a RDMA switch with 40Gbps costs about 
16K US \$. RDMA provides three types of communication primitives, including 
IPoIB, message verbs, and one-sided read/write operations, from slow to fast. To 
perform RDMA operations, the process and the remote process establishes a 
communication end point called Queue Pairs (QP). The remote memory access is 
fully operated by hardware without involing software network layers, OS kernel, 
or CPU of the remote machine. QP are lossless in normal case, but packet losses 
may happen during machine or software (\eg, the server program) restarts.

One-side RDMA operations can totally write from one machine's memory to a 
remote machine's memory directly. However, for one-sided operations, the remote 
machine's memory is not aware of the write either, so a careful protocol design 
is necessary when one-sided operations are used.


\section{\xxx Overview}\label{sec:overview}

This section presents an overview of \xxx's architecture with its deployment 
model and key components (\S\ref{sec:arch}), and it gives an example to show 
how it works (\S\ref{sec:example}).

\subsection{Architecture}\label{sec:arch}

\begin{figure*}[!htb]
\centering
\includegraphics[width=0.5\textwidth]{figures/arch}
\vspace{-.10in}
\caption{{\em The \xxx Architecture.} \rm {\xxx components are shaded (and in
  green).}} \label{fig:arch}
\vspace{-.05in}
\end{figure*}

% TBD. Input coordinator and output checker. Components.

% System model. Replicas. RDMA. LAN. Clients.
To replicate a server program, \xxx is deployed in a datacenter, with a set of 
three or five replicas connecting with RDMA architecture, InfiniBand. Client 
programs located in LAN or WAN network and send client requests to the leader 
machine as if only one server program is running. If clients send requests to 
the wrong machines, \xxx's backup machines deny the requests and reply the 
leader's IP.

Figure~\ref{fig:arch} shows \xxx's architecture. \xxx intercepts four types of 
socket operations: the \accept type, the \recv type, the \send type, and the 
\close type. These types invoke five key components, the input coordination 
protocol (for short, \emph{coordinator}), the output checking protocol (the 
\emph{checker}), the follower thread (\emph{follower}), the checkpoint helper 
thread (\emph{checkpoint helper}), and the \emph{guard} (for checkpointing and 
restoring a server program's process state and file system state). The follower 
is only active on backup replicas.

% Input coordination protocol.
Once the server program in the leader machine receives a client request 
connenction, \xxx intercepts the \accept function call in the server side in 
all replicas and ask the input coordinator to invoke consensus on building a 
new connection. Once a majority of replicas agree, the leader machine returns 
from the \accept call, and the replica's machine's follower sets up a new
connection between this thread and the server.

When a client program sends a actual request to the server, the leader's server 
traps into the \recv function call and invoke consensus on this new input. If 
a consensus is made, the leader directly returns from this function call and 
processes the request, and the backup's follower forwards this request to the 
server program on its own machine.

% Output checking protcol.
% Challenge: what if leader does more sends and replicas do fewer sends?
When a request is processed and the server is about to send reply to client, 
\xxx traps into the \send call, computes historical hash value on inputs when 
4K bytes (MTU size), and then periodically invoke the output coordination 
protocol. All backups return consensus reply to the leader with their own hash 
values if this value is available on their replica.

If divergent hash is detected, XXX. Guard.

% follower

% checkpoint helper.

% The guard component. Checkpoint and restore.


\subsection{Example}\label{sec:example}

% TBD. A simple server with recv(), accept(), and send(). Must have a 
% concurrency bug in this toy program? Race on global var or heap?