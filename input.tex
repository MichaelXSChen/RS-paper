\section{Input Consensus Protocol} \label{sec:input}

This section first presents \xxx's RDMA-accelerated \paxos consensus protocol, 
including the fast,scalable algorithm in normal case (\S\ref{sec:normal}), 
handling concurrent connections (\S\ref{sec:concurrent}), and the leader 
election protocol (\S\ref{sec:election}). It then analyzes the reliability of 
our protocol (\S\ref{sec:guarantees}).

\subsection{Normal Case Algorithm} \label{sec:normal}

\begin{figure}[t]
\centering
\begin{minipage}{.5\textwidth}
\lgrindfile{code/logentry.cpp}
\end{minipage}
\vspace{-.1in}
\caption{{\em \xxx's log entry for each socket call.}} \label{fig:logentry}
\vspace{-.05in}
\end{figure}

\begin{figure}[t]
\centering
\vspace{-0.15in}
\includegraphics[width=.48\textwidth]{figures/consensus}
\vspace{-.4in}
\caption{{\em \xxx consensus algorithm in normal case.}} \label{fig:consensus}
\vspace{-.05in}
\end{figure}

% \subsubsection{The Basic } \label{sec:primitive}

% Question: What if on a machine, it viewed it as the leader, it executes the 
% actual socket call, and then it invokes the consensus, but then it is no 
% longer the leader any more. How to undo the actual socket calls? Cases:
  %% On accept(). The old leader has already accepted and asks for consensus.
  %% On recv(). The old leader has already received.
  %% On send().
  %% On close().

% First, basic roles. log format details.
Recall that \xxx' input consensus protocol contains three roles. First, the 
\paxos consensus log (\S\ref{sec:overview}). Second, a leader replica's server 
program thread (in short, a leader thread) which invokes consensus request. For 
efficiency, \xxx lets a server program's threads directly handle consensus 
requests whenever they call inbound socket calls (\eg, \recv). Third, a backup 
replica's \xxx internal follower thread (\S\ref{sec:overview}) which agrees on 
or rejects consensus requests.

Figure~\ref{fig:logentry} shows the format of a log entry in \xxx's consensus 
log. Most fields are regular as those in a typical \paxos 
protocol~\cite{paxos:practical} except three ones: the \v{ack} array, the 
client connection ID \v{conn\_vs}, and the type ID of a socket call 
\v{call\_type}. The \v{ack} array is for backups to WRITE their consensus 
replies to the leader. The \v{conn\_vs} is for identifying which connection this 
socket call belongs to (see \ref{sec:concurrent}). The \v{call\_type} 
identifies four types of socket calls in \xxx: the \accept type (\eg, \accept), 
the \recv type (\eg, \recv and \myread), the \send type (\eg, \send and 
\mywrite), and the \close type (\eg, \close).

% Second, leader thread behavior on \recv(). Return from orig recv, grab 
% spin % lock and get view stamp, store the log, and post send to all replicas. 
% Non blocking. Wait % for over half agree, and then execute the operation.
Figure~\ref{fig:logentry} shows the input consensus algorithm. Suppose a leader 
thread invokes a consensus request when it calls a socet call with the \recv 
type. A consensus request includes four steps. The first step (\textbf{L1}) is 
executing the actual Libc socket call, because \xxx needs to get the actual 
return values or received data bytes of this call and then replicates them in 
remote replicas' logs.

The second step (\textbf{L2}) is local preparation, including assigning a 
global, monotonically increashing viewstamp to locate this entry in the 
consensus log, building a log entry struct for this call, and writing this 
entry to its local SSD.

The third step (\textbf{L3}) is to WRITE a log entry to remote backups 
in parallel. Unlike a previous RDMA-based consensus 
algorithm~\cite{dare:hpdc15} which has to wait for ACKs from remote NICs, our 
WRITE immediately returns after pushing the entry to its local QP between the 
leader and each backup, because \paxos has handled the reliability issues (\eg, 
packet losses) for our WRITEs. In our evaluation, pushing a log entry to local 
QP took no more than 0.1 \us. Therefore, the WRITEs to all backups are done in 
parallel (see \textbf{L3} in Figure~\ref{fig:logentry}).

% Instead, \xxx's leader thread just return 
% immediately after 
% putting the structs to the RDMA QP (Queue Pair) between a backup replica, 
% because \paxos protocol \xxx implements has already considered packet losses 
% (\eg, due to remote replica failures).

The fourth step (\textbf{L4}) is the leader thread polling on its \v{ack} 
field (notably, different from an RDMA ACK) in its local log entry for backups' 
consensus replies. Once consensus is reached, the leader thread finishes 
intercepting this \recv socket call and continues with its server application 
logic.

% 
% The leader does a
% busy loop to check the \v{ack} array until it sees that a majority of replicas, 
% including itself, have aggreed on this log entry. In normal case, both the 
% third and fourth steps will return immediately because no synchronization 
% context switch is involved.

% Post send carry the last committed (consensus reached) operation.

% One key issue, check integrity. Strawmen approach, check viewstamp first.
On a backup side, one tricky synchronization issue is that an efficient way is 
needed to make the leader's RDMA WRITEs and backups' polls atomic. For instance, 
while a leader thread is doing a WRITE on \v{vs} to a remote backup, this 
backup's follower thread may be reading this variable concurrently, causing a 
incomplete (wrong) read.

To address this issue, one existing approach~\cite{farm:nsdi14,herd:sigcomm14} 
leverages the left-to-right ordering of RDMA WRITEs and puts a special 
non-zero variable at the end of a fix-sized log entry because they mainly 
handle key-value stores with fixed value length. As long as this variable is 
non-zero, the RDMA WRITE ordering guarantees that the log entry WRITE is 
complete. However, because \xxx aims to support general server programs with 
largely variant received data lengths, this approach cannot be applied in \xxx.

An alternative approach is using atomic primitives provided by RDMA hardware, 
but a prior evaluation~\cite{drtm:sosp15} has showned that RDMA atomic 
primivites are much slower than normal RDMA WRITEs and local memory reads.
% Thus, in a RDMA-accelerated protocol, an extra mechanism 
% is needed to gurantee that a 
% leader's write has finished and then replicas can agree or disagree.

\xxx tackles this issue by adding a canary value after the actual \v{data} 
array. Because \xxx uses a QP with the type of RC (reliable connection) 
(\S\ref{sec:background}), the follower always first checks the canary value 
according to \v{data\_size} and then starts a standard \paxos consensus reply 
decision~\cite{paxos:practical}. Our efficient, synchronization-free approach 
guarantees that the follower always reads a complete log entry.
% This check guarantees that a log entry is 
% completely written in a local backup.
% 
% To ahchieve small consensus latency, \xxx's follower thread does a busy loop 
% on a dedicated CPU core to agree on consensus requests from the leader. Each 
% backup only needs one follower thread. This thread always busy reading the 
% latest un-agreed log entry in its local log, and it it sees a log entry has 
% completely written, it runs three steps. 

A follower thread in a backup replica polls from the latest un-agreed log 
entry and does three steps to agree on consensus requests, shown in 
Figure~\ref{fig:consensus}. First (\textbf{B1}), it does a regular \paxos view 
ID checking to see whether the leader is up-to-date, it then stores the log 
entry in its local SSD. Second (\textbf{B2}), it does a WRITE to send back 
a consensus reply to the leader's \v{ack} array element according to its 
own node ID. Backups perform these two steps in parallel 
(see Figure~\ref{fig:logentry}).

Third (\textbf{B3}, not shown in Figure~\ref{fig:consensus}), the follower 
does a regular \paxos check on \v{last\_committed} and executes all socket 
calls that it has not executed before this viewstamp. It then ``executes" each 
log entry by forwarding the socket calls to the local server program. This 
forwarding faithly builds and closes concurrent connections between the follower 
and the local server program according to the socket calls in the consensus log.
% thus \xxx can 
% maintain consistent concurrent client connection mappings between the leader and 
% backups.


% TBD: should we put the impl here or somewhere else?
In a implementation level, \xxx stores log entries in local SSD using Berkely 
DB~\cite{berkeleydb}. Although \xxx's algorithm does not wait every RDMA ACK in 
order to achieve high scalability, we use selective signaling 
(\S\ref{sec:background}) to occasionally check and clear ACKs in the CQ 
associated with the QP, an essential ACK clearing step in RDMA implementations.

% On replicas, server programs run as is without being 
% intercepted. In short, this follower thread runs a high performance loop to 
% respond consensus requests and forward data to the local server program. Since 
% each backup machine only has one follower thread and nowadays machines often 
% have spare cores, we didn't find that the spin loop of this thread brought 
% bring negative performance impact in our evaluation.

% Performance: two RDMA writes and two SSD stores. No context switches.
% This protocol is highly optimized for minimizing consensus latency in normal 
% case. In total, a consensus between the leader and one backup only requires 
% two one-sided RDMA write operations (one from the leader to the backup and the 
% other from the backup to the leader) and two SSD write operations (each in 
% leader and backup). Although each RDMA one-sided operation takes about 3 \us, 
% \xxx's protocol just puts the log entry to the RDMA Queue Pair without needing 
% to wait until the write succeeds on the remote backup, because the leader will 
% have \paxos's consensus reply (the RDMA write from the backup to leader). In 
% addition, neither a leader thread or a follower thread does a synchronization 
% context switch during this consensus (a synchronization context switch 
% typically takes sub milli seconds, pretty slow).

% Question: one key question, can RDMA lose packets in between? I mean, both 
% data_size and canary value are correct, but packets in the middle are lost. 
% This may violate \paxos's non-corrupting packets assumption.

% Third, replica thread behavior on \recv(). Block on latest un agreed log, 
% wait for the % consensus log, check integrity, check view id, and then store to 
% BDB. and % then execute the committed but not executed requests from BDB.

\subsection{Handling Concurrent Connections} \label{sec:concurrent}

Unlike traditional \paxos protocols which mainly handle single-threaded 
programs due to SMR's deterministic state machine assumption, \xxx aims to 
support both single-threaded as well as multithreaded server programs running 
on multi-core machines. Therefore, a strongly consistent mechanism is needed to 
map every concurrent client connection on the leader and to its corresponding 
connection on backups. A naive approach could be matching a leader connection's 
socket descriptor to the same one on a backup, but backups' servers may return 
nondeterministic descriptors due contentions on systems resources.

Fortunately, \paxos have already made viewstamps of socket calls strongly 
consistent across replicas. For TCP connections, \xxx addes the 
\v{conn\_vs} field, the viewstamp of the the first socket call in each 
connection (\ie, \accept) as the connection ID for log entries. Then, \xxx 
maintains a hashmap on each local replica to map this connection ID to local 
socket descriptors.

% On a local 
% replica, \xxx maintains a hash map between this connection ID 
% to the actual local socket descriptor for each connection, then \xxx ensures 
% that data bytes are forwarded from a leader's connection to the right matching 
% connection on backups. \xxx also intercepts socket calls with the \close type 
% to clean up this map. For servers that use UDP to serve requests, \xxx maps the 
% viewstamp of a \recvfrom call to the socket descriptor returned from this call, 
% and it cleans up the map on a corresponding \sendto call. 

% TBD. Strawmen approach, use fds on leader machines.

%% thread ID mapping is not good too, because accept and recv can be called by 
% different threads in leader side.

% Use viewstamp of the accept() operation, consistent across replicas.


% What does replica thread do. High performance loop on a dedicated core.

\subsection{Leader Election} \label{sec:election}

% % Use one log operation or a few operations to do so. Clean, 
% match the PMS intention. Explain why this design choice is good.

% TBD. Four steps. Four entries?
Compared to traditional \paxos leader election protocols, RDMA-based 
leader election poses one main issue caused by RDMA. Because backups do not 
communicate frequently with each other in normal case, thus a backup does not 
know the remote memory locations where the other backups are polling. Writing to 
a wrong remote memory location may cause the other backups to miss all leader 
election messages. An existing approach establishes a control QP with specific 
remote memory to handle leader election, but it poses complexity on the extra 
QPs and specific data structures.

% If current leader fails, 
% how does a backup know the other remote backups' memory location to WRITE to? 

% Mention old leader.
% The second issue still stems from RDMA's nature: while a leader election is 
% on going, the out-dated leader may still WRITE to remote backups and corrupt 
% the ongoing leader election. This WRITE will not get a Yes consensus reply 
% because the other replicas can transit their current state from active to a 
% leader election state~\cite{paxos:practical}, but the old leader's WRITE may 
% still corrupt the ongoing leader election. 

For simplicity, \xxx handles election election with the same consensus log. It 
addresses this issue with a simple approach. The leader does WRITEs to remote 
logs as heartbeats with a period of T. Once backups have not received heartbeats 
from the leader for a period of 3*T, they start to elect new leader. Before 
starting a standard leader election phase as in tranditional \paxos 
protocols~\cite{paxos:practical}, each backup always first does RDMA one-sided 
reads to get remote replicas' current polling location.

Then, backups start a standard \paxos leader 
election algorithm~\cite{paxos:practical} on consensus logs with three steps. 
First, backups propose a new view with a standard two-round \paxos 
consensus~\cite{paxos:simple} by including its current log entry offset. 
The backup with largest offset have a priority to win this proposal. Second, the 
winning proposer proposes itself as a primary candidate, another two-round 
\paxos consensus. Third, if the second step reaches a quorum, the new leader 
notifies remote replicas itself as the new leader and it starts to WRITE 
periodic heartbeats.

\subsection{Reliability} \label{sec:guarantees}

% Follow PMP, ease to understand, absorbe good exeprience.
% \paxos is notoriously difficult to 
% understand~\cite{raft:usenix14,paxos:simple,paxos,paxos:complex} or 
% implement~\cite{paxos:live,paxos:practical}. 
% 
To minimize protocol-level bugs, 
\xxx's \paxos protocol mostly sticks with a popular, 
practical implementation~\cite{paxos:practical}, espeically the behaviors of 
senders and receivers (\S\ref{sec:normal} and \S\ref{sec:election}). For 
instance, both \xxx's normal case algorithm and 
the popular implementation~\cite{paxos:practical} involve two messages and 
same senders and receivers (although we use WRITEs and carefully make them run 
in parallel). We made this choice 
because \paxos is \paxos is notoriously difficult to 
understand~\cite{raft:usenix14,paxos:simple,paxos,paxos:complex} or 
implement~\cite{paxos:live,paxos:practical} 
verify~\cite{modist:nsdi09,demeter:sosp11}. Aligning with a practical
\paxos implementation~\cite{paxos:practical} helps us incorporate these 
readily mature understanding, engineering experience, and the theoretically 
verified safety rules into our protocol design and implementation.

% TBD: reliability analyzes. We do not need the lossless assumption in RDMA QPs.
Although \xxx's \paxos protocol works on a RDMA network, the reliability of 
this protocol does not rely on the lossless networking in RDMA. \xxx's protocol 
still complies with the standard \paxos failure-handling model, where a stable 
storage exists, but hardware may fail, network may be partitioned, packets may 
be delayed or lost, and server programs may crash.


% Define safety: TBD.
% A \paxos protocol must ensure safety: the agreed operation must be an actually 
% proposed one; if agreed, all active replicas must consistently enforce this 
% operation. Safety is another important sweet spot that \xxx's input 
% coordination protocol inherites from traditional \paxos protocol by sticking 
% with same replica behavior in traditional ones. As a traditional protocol, 
% our input coordination protocol also uses view IDs and viewstamps to enforce a 
% consistent, up-to-date leadership across replicas. To address the atomicy issue 
% between remote RDMA writes and local memory reads, our protocol addes an 
% completion check on log entries (\S\ref{sec:normal}).
% In short, our 
% design choice makes our input coordination protocol enforce the same level of 
% safety as traditional \paxos protocols~\cite{paxos, paxos:simple, 
% paxos:practical}.

% Safety, viewstamp.

% Store to stable storage.

% Any unique changes that prevent us from providing guarantees? The delta 
% changes? Packets. Logs?

% \begin{figure}[t]
% \centering
% \vspace{-.20in}
% \includegraphics[width=.48\textwidth]{figures/dare}
% \vspace{-.20in}
% \caption{{\em DARE consensus algorithm in normal case.}} \label{fig:dare}
% \vspace{-.05in}
% \end{figure}
% 
% We compare the performance and scalability between \xxx and DARE, the fastest 
% RDMA-based consensus algorithm we know so far. As shown on 
% Figure~\ref{fig:consensus}, a complete \xxx consensus round consists of three 
% parts: a leader's local SSD store operation (L2), a backup's local SSD store 
% operation (B1), and a WRITE round-trip (L3 and B2). Given the same inputs, 
% the latency of these three parts are constant, thus \xxx's consensus latency is 
% not approximately constant to replica group size. \xxx's scalability is mainly 
% bounded by the max number of outbound RDMA writes in the RDMA NIC hardware 
% (currently, 16~\cite{herd:sigcomm}).
% 
% We carefully examined DARE's code base and depict its consensus algorithm in 
% Figure~\ref{fig:dare}, which consists of three parts: log entry WRITEs to all 
% remote logs' with ACKs, log tail pointer WRITEs to all remote logs with ACKs, 
% and a quorum notification by WRITEs to agreed remote replicas without ACKs. In 
% DARE's pure leader-based consensus, ACKs are necessary for every next step to 
% start. Therefore, DARE's consensus latency is linear to replica group size. We 
% show concrete consensus latency comparisons of \xxx and DARE in evaluation 
% (\S\ref{sec:scalability}).