\section{Evaluation} \label{sec:evaluation}

\begin{table}[b]
\footnotesize
\centering
\vspace{-.15in}
\begin{tabular}{lrr}
{\bf Program} & {\bf Benchmark} & {\bf Workload/input description}\\
\hline\\[-2.3ex]
\clamav & clamscan~\cite{clamscan}  & Files in \v{/lib} from a replica \\
\mediatomb & ApacheBench~\cite{apachebench}  & Transcoding videos\\
\memcached & mcperf~\cite{mcperf}  & 50\% set, 50\% get operations\\
\mongodb & YCSB~\cite{ycsb}  & Insert operations\\
\mysql & Sysbench~\cite{sysbench}  & SQL transactions\\
\openldap & Self  & LDAP queries\\
\redis & Self  & 50\% set, 50\% get operations\\
\ssdb & Self  & Eleven operation types\\
\calvin & Self  & SQL transactions\\
\end{tabular}
\vspace{-.05in}
\caption{{\em Benchmarks and workloads.} ``Self" in the Benchmark column means
we used a program's own performance benchmark program. Workloads are all
concurrent.}
\label{tab:benchmarks}
\end{table}

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth]{figures/throughput}
\vspace{-.10in}
\caption{\small {\em \xxx throughput compared to the unreplicated
execution.}}
\vspace{-.20in}
\label{fig:tput}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth]{figures/latency}
\vspace{-.10in}
\caption{\small {\em \xxx response time compared to the unreplicated
execution.}}
\vspace{-.20in}
\label{fig:latency}
\end{figure*}

Our evaluation machines include nine RDMA-enabled, Dell R430 servers as \paxos 
replicas. Each server has Linux 3.16.0, 2.6 GHz Intel Xeon CPU with 24 
hyper-threading cores, 64GB memory, and 1TB SSD. All NICs are Mellanox 
ConnectX-3 Pro Dual Port 40 Gbps, connected via Infiniband~\cite{infiniband}. 
The \v{ping} latency between every two replicas are 84 \us (the IPoIB 
round-trip latency).
%
% through a Dell S6000 high-performance switch with 32 40Gpbs ports.

Our evaluation machines also include one Dell R320 server for client programs. 
It has Linux 3.16.0, 2.2GHz Intel Xeon 12 hyper-threading cores, 32GB memory, 
and 160GB SSD. To mitigate latency of client requests, this client machine is 
located at the same LAN as the RDMA replicas with a 1Gbps NIC. The average 
<<<<<<< HEAD
\v{ping} latency between this machine and a RDMA replica is 241 \us. Running 
cilents on which machine does not affect any \paxos protocol's consensus 
=======
\v{ping} latency between this machine and a RDMA replica is 249 \us. Note that 
which machines to run cilents on do not affect any \paxos protocol's consensus 
>>>>>>> 93b2c0d1979b9ba4d0f942586352b3928b427096
latency (it is only affected by the RDMA network among replicas).

% A larger 
% network latency (\eg, sending client requests from WAN) will further mask 
% \xxx's overhead.

We compared \xxx with five popular, open source \paxos-like implementations,
including four traditional ones (\libpaxos~\cite{libpaxos},
\zookeeper~\cite{zookeeper}, \crane~\cite{crane:sosp15} and
\spaxos~\cite{spaxos}) and a RDMA-based one (\dare~\cite{dare:hpdc15}). \spaxos
is designed to achieve scalable throughput when more replicas are added. 

We evaluated \xxx on \nprog widely used or studied server programs, including
\nkvprog key-value stores \redis, \memcached, \ssdb, \mongodb; \mysql, a SQL
server; \clamav, an anti-virus server that scans files and delete malicious ones;
\mediatomb, a multimedia storage server that stores and transcodes video and
audio files; \openldap, an LDAP server; \calvin, a widely studied transactional
database system. All these programs are multithreaded except \redis (but it can 
serve concurrent requests via Libevent). These servers all update or store 
important data and files, thus the strong \paxos fault-tolerance is especially 
attractive to these programs.

% Benchmarks table.
Table~\ref{tab:benchmarks} introduces the benchmarks and workloads we used. To
evaluate \xxx's practicality, we used the protocol or program developers' own 
performance benchmarks or popular third-party benchmarks. For benchmark 
workload settings, we used the benchmarks' default workloads whenever 
available. To perform a stress testing on \xxx's input consensus protocol, we 
chose workloads with significant portions of writes, because write operations 
often contain more input bytes than reads (\eg, a key-value SET operation 
contains more bytes than a GET).



% evaluation metric. client benchmarks all run in LAN, average latency
The rest of this section focuses on these questions:

\begin{tightenum}

\item[\S\ref{sec:eval-traditional}:] What is \xxx's consensus latency compared
to traditional \paxos protocols?

\item[\S\ref{sec:eval-dare}:] What is \xxx's consensus latency compared
to \dare?

\item[\S\ref{sec:overhead}:] What is the performance overhead of running \xxx
with general server programs? How does it scale with concurrent requests?

% \item[\S\ref{sec:scalability}:] How scalable is \xxx on different replica group
% sizes?

\item[\S\ref{sec:robust}:] How fast is \xxx on handling checkpoints and
electing a new leader?

% \item[\S\ref{sec:race}:] If some \xxx users care about data races much, how
% does \xxx tolerate the slowdown of data race detector by deploying it on a
% replica?

% \item[\S\ref{sec:lesson}:] What practical lessons have we learnt during the
% case study on these server programs with \xxx?

\end{tightenum}




\subsection{Comparing with Traditional \paxos}
\label{sec:eval-traditional}

As a common evaluation practice in 
\paxos systems~\cite{dare:hpdc15,nopaxos:osdi16}, when comparing \xxx with 
other \paxos protocols, we ran \xxx with a popular key value store \redis. For 
all six \paxos protocols, we spawned 20 consensus requests, a common high 
concurrent value in prior 
evaluation~\cite{zookeeper,crane:sosp15,rex:eurosys14}. Our evaluation also 
showed that most server programs reached peak performance at this concurrent 
value (\ref{sec:overhead}).

We compared \xxx with four traditional protocols, \libpaxos~\cite{libpaxos},
\zookeeper~\cite{zookeeper}, \crane~\cite{crane:sosp15} and
\spaxos~\cite{spaxos}. Figure~\ref{fig:scalability} has already shown the 
results. Three traditional protocols incurred almost a linear increase of 
consensus latency except \spaxos. \spaxos batch requests from replicas and it 
only invoke consensus when a fixed batch is full. More replicas will make this 
batch be full more quickly, so \spaxos incurred slightly better consensus 
latency at more nodes. Nevertheless, its consensus latency was over 700 \us. 
\xxx's consensus latency outperforms these protocols by \comptradlow to 
\comptradhigh on 3 to 9 replicas. Therefore, we needn't run these traditional 
protocols on more replicas.

To understand why traditional protocols are unscalable, we ran 
only one client with them and inspect the micro events in their protocols, 
shown in Table~\ref{tab:traditional-latency}. Three protocols had scalable 
latency on the arrival of their first consensus reply (the ``First" column), 
which means that network bandwith is not a scalability bottleneck for them.  
\libpaxos is an exception because its two-round protocol consumed much 
bandwidth. However, there is a big gap between the arrival of the first 
consensu reply and the ``majority" reply (the ``Major" column). Given that the 
reply CPU processing time was small (the ``Process" column), we can see that 
the various systems layers, including OS kernels, network libraries, and 
language runtimes (\eg, JVM) are the scalable bottleneck (the ``Sys" column). 
This indicates that RDMA is useful to bypass the systems layers for better 
scalability.

Note that when running with three replicas, \libpaxos and \crane's proposing 
leader and acceptors are in different threads, so they two had different 
``First" and ``Major" arrival times. \crane and \spaxos's proposing 
leader itself is just an accepter, so they two had same ``First" and ``Major" 
arrival times (\ie, their ``Sys" times were negligible).

% We compared \xxx with \calvin's SMR system because \calvin's input consensus
% uses \zookeeper, one of the most widely used coordination service built on
% TCP/IP. To conduct a fair comparison, we ran \calvin's own transactional
% database server in \xxx as the server program, and we compared throughputs and
% the consensus latency with \calvin's consensus protocol \zookeeper.

% As shown in Table~\ref{tab:compare}, \calvin's \zookeeper replication achieved
% 19.9K transactions/s with a 511.9 \us consensus latency. \xxx achieved 18.9K
% transactions/s with a 19.5 \us consensus latency. The throughput in \calvin
% was 5.3\% higher than that in \xxx because \calvin puts transactions in a
% batch with a 10 \ms timeout, it then invokes \zookeeper for consensus on
% this batch. The average number of bytes in \calvin's batches is 18.8KB, and
% the average number of input bytes in each \xxx consensus (one for each \myread
% call) is 128 bytes. Batching helps \calvin achieve good throughput. \xxx
% currently has not incorporated a batching technique because its latency is
% already reasonable (\S\ref{sec:overhead}).
% 
% Notably, \xxx's consensus latency was 40.1X faster than \zookeeper's mainly due
% to \xxx's RDMA-accelerated consensus protocol, although we ran \calvin's
% \zookeeper consensus on IPoIB. A prior SMR evaluation~\cite{dare:hpdc15} also
% reports a similar 320 \us consensus latency in \zookeeper. Two other recent SMR
% systems Crane~\cite{crane:sosp15} and Rex~\cite{rex:eurosys14} may incur
% similar
% consensus latency as \zookeeper's because all their consensus protocols are
% based on TCP/IP. Overall, this \xxx-\calvin comparison suggests that \calvin
% will be better if clients prefer high throughput, and \xxx will be better if
% clients demand short latency.
% TBD: rerun \redis micro events; it is too small, so we can not use it here.

\begin{table}[h]
\footnotesize
\centering
\vspace{.05in}
\begin{tabular}{lrrrrr}
{\bf Proto-\#Rep} & {\bf Latency} & {\bf First} & {\bf Major} & {\bf
Process}
& {\bf Sys}\\
\hline\\[-2.3ex]
\libpaxos-3 & 81.6 & 74.0  & 81.6 & 2.5 & 5.1\\
\libpaxos-9 & 208.3 & 145.0  & 208.3 & 12.0 & 51.3\\

\hline\\[-2.3ex]
\zookeeper-3 & 99.0 & 67.0  & 99.0 & 0.84 & 30.3\\
\zookeeper-9 & 129.0 & 76.0  & 128.0 & 3.6 & 49.4\\

\hline\\[-2.3ex]
\crane-3 & 78.0 & 69.0  & 69.0 & 13.0 & 0.1\\
\crane-9 & 148.0 & 83.0  & 142.0 & 30.0 & 35.0\\

\hline\\[-2.3ex]
\spaxos-3 & 865.1 & 846.0  & 846.0 & 20.0 & 0.1\\
\spaxos-9 & 739.1 & 545.0  & 731.0 & 35.0 & 159.1\\

\end{tabular}
\vspace{-.05in}
\caption{{\em Scalability bottleneck analysis in traditional \paxos protocols.}
The ``Proto-\#Rep" column means the \paxos protocol name and replica group
size; ``Latency" means the consensus latency; ``First" means the latency
of its first consensus reply; ``Major" means the
latency of its the majority reply; ``Process" means time spent in
processing all replies; and ``Sys" means time spent in systems (OS
kernel, network libraries, and JVM) between the ``First" and the ``Major" 
reply. All times are in \us.}
\label{tab:traditional-latency}
\end{table}

\subsection{Comparing with \paxos}
\label{sec:eval-dare}

\begin{table}[h]
\footnotesize
\centering
% \vspace{-.1in}
\begin{tabular}{lrrrrr}

{\bf \# Replicas} & {\bf 3} & {\bf 9} & {\bf 33} & {\bf 65} & {\bf 105} \\
\hline\\[-2.3ex]
\xxx (update-heavy) & 8.2 & 8.8 & 13.0 & 20.3 & 31.6 \\

\hline\\[-2.3ex]
\dare (update-heavy) & 8.8 & 12.0  & 32.5 & 64.0 & 102.5 \\

\hline\\[-2.3ex]
\dare (read-heavy) & 5.8 & 7.2 & 16.8 & 29.2 & 45.1 \\

\end{tabular}
\vspace{-.05in}
\caption{{\em Consensus latency of \xxx and \dare.} The update-heavy workload 
consist of 50\% SETs. The read-heavy workload consist of 10\% SETs.}
\label{tab:consensus-latency}
\vspace{-.2in}
\end{table}

We first compared \xxx and \dare~\cite{dare:hpdc15} with the key-value 
stores. \xxx uses a widely used one, \redis; \dare uses 335-line key-value 
store written by their authors. To thoroughly analyze their latency with 
scalability, we run more replicas than the nine physical machines (\ie, each 
machine runs several replicas). Since our RDMA NIC bandwidth is 40Gpbs, we 
didn't find network a bottleneck when running more replicas on a machine; from 9 
to 105 replicas for both protocols, each RDMA round-trip increased merely from 
2.6$\sim$4.4 \us.

Table~\ref{tab:consensus-latency} shows the consensus latency of \xxx and 
\dare on the same update-heavy workload, which contains half GETs and half 
SETs. This workload represents real-world applications such as an advertisement 
log that records recent user activities~\cite{dare:hpdc15}. \xxx and \dare 
achieves similar latency at three replicas, and \xxx was much faster than \dare 
on more replicas. Their difference is 2.5x$\sim$3.3x on over 33 replicas. When 
changing replica group size from 3 to 105 (a 35x increase), \xxx's consensus 
latency merely increased by 3.8x, while \dare increased by 11.7x.

\xxx scales better than \dare for two main reasons. First, in a protocol level, 
\xxx's protocol carefully separate the RDMA workloads across leader and 
backups, and it is a one-round protocol (\S\ref{sec:normal}). \dare lets its 
leader do all the consensus work and backups do nothing, and it is a two-round 
protocol (\S\ref{sec:compare}). \dare involves approximately 2x more RDMA 
communications than \xxx.

Second, in a RDMA communication primitive level, \xxx lets all replicas receive 
consensus messages on their bare, local memory. \dare frequently polls 
RDMA ACKs from a RDMA Completion Queue (CQ) in each consensus round. An ACK 
polling or insertion operation on the CQ involves synchronization between 
RDMA NICs among replicas. We found that although \dare's ACK mechanism is 
already highly optimized with a global CQ, so that it can poll more ACKs at 
one time. However, we found that ACKs arrived randomly, and each polling in 
\dare took up to 410 \us (\S\ref{sec:background}). The more replicas, the more 
ACK polling operations were needed.

\xxx does the same consensus round on all types of requests. \dare does two 
special handling on GET requests. First, it batchs GET requests for consensus, 
which may improve throughput but aggravate latency. Second, \dare's GET 
requests only involve one-round consensus (does RDMA reads to fetch all 
replicas's \paxos view IDs~\cite{paxos:practical} and check if a majority of 
them match the leader's). We also ran \dare with a read-heavy workload 
(Table~\ref{tab:consensus-latency}) and its consensus latency was about 50\% 
slower than \xxx on large replica groups. Note that \xxx has a durable input 
storage, and \dare is a volatile protocol. Overall, we considered \xxx faster 
and more scalable than \dare.

% Table~\ref{tab:consensus-latency} shows the Scalability results. Given the same
% workload, \xxx's five-replica setting had a 17.9K requests/s and 10.1
% \us consensus latency, and its seven-replica setting had a 17.4K requests/s and
% 11.0 \us consensus latency. Compared to the three-replica setting, \xxx
% achieved a similar consensus latency from three to seven replicas, because a
% \xxx consensus latency mainly contains two SSD stores and one WRITE round-trip
% (see Table~\ref{tab:consensus-latency}).
% 
% We didn't consider these initial scalability results general; we found them
% promising. Given that each RDMA NIC hardware port supports up to 16 outbound
% RDMA WRITEs with peak performance~\cite{herd:sigcomm14}, and our NIC has dual
% ports, we anticipate that our algorithm may scale up to around 32 replicas
% under current hardware techniques. We plan to buy more servers and verify
% whether \xxx's scalability is bounded by RDMA NIC capacity. \S\ref{sec:comp}


\subsection{Performance Overhead} \label{sec:overhead}

To stress \xxx, we used a large SMR group size of 9 to run all server programs. 
We spawned up to 32 concurrent connections, and then we measured both response 
time and throughput. We also measured \xxx's bare consensus latency. Each 
performance data point in the evaluation is taken from the mean value of 
10 repeated executions.

% \subsection{Ease of Use} \label{sec:ease-of-use}

\xxx is able to run all \nprog evaluated programs without modifying them except
\calvin. \calvin integrates its client program and server program within the
same process and uses local memory to let these two programs communicate. To
make \calvin's client and server communicate with POSIX sockets so that \xxx
can intercept the server's inputs, we wrote a \nlinescalvin-line patch for
\calvin.

Figure~\ref{fig:tput} shows \xxx's throughput and Figure~\ref{fig:latency}
response time. We varied the number of concurrent client connections for each
server program by from one to 32 threads. For \calvin, we only collected the
8-thread result because \calvin uses this constant thread count in their code
to serve client requests. Overall, compared to these server programs'
unreplicated executions, \xxx merely incurred a mean throughput overhead of
\tputoverhead (note that in Figure~\ref{fig:tput}, the Y-axises of most programs
start from a large number). \xxx's mean overhead on response time was merely
\latencyoverhead.

As the number of threads increases, all programs' unreplicated executions
got a performance improvement except \memcached. A prior
evaluation~\cite{rex:eurosys14} also observed a similar \memcached low
scalability. \xxx scaled almost as well as the unreplicated executions.
% a \latencyoverhead overhead compared to the unreplicated
% executions.

\xxx achieves such a low overhead in both throughput and response time mainly
because of two reasons. First, for each \recv call in a server, \xxx's input
coordination protocol only contains two one-sided RDMA writes and two SSD writes
between each leader and backup. A parallel SSD write
approach~\cite{Bessani:usenix13} may further improve \xxx's SSD performance.
Second, \xxx's output checking protocol invokes occasionally
(\S\ref{sec:output-workflow}).

\begin{table}[h]
\footnotesize
\centering
\vspace{-.05in}
\begin{tabular}{lrrrr}
{\bf Program} & {\bf \# Calls} & {\bf Input} & {\bf SSD time}
& {\bf Quorum time}\\
\hline\\[-2.3ex]
% Heming: normalized clamav to 10K req.
\clamav & 30,000  & 37.0 & 7.9 \us & 10.9 \us\\
% TBD: normalize mediatomb to 10K req.
\mediatomb & 30,000  & 140.0 & 5.0 \us & 17.4 \us\\
\memcached & 10,016  & 38.0 & 4.9 \us & 7.0 \us\\
\mongodb & 10,376  & 490.6 & 7.8 \us & 9.2 \us\\
\mysql & 10,009  & 28.8 & 5.1 \us & 7.8 \us\\
% TBD: normalize ldap to 10K req.
\openldap & 10,016  & 27.3 & 5.5 \us & 6.4 \us\\
\redis & 10,016  & 40.5 & 2.8 \us & 6.3 \us\\
% TBD: normalize ssdb to 10K req.
\ssdb & 10,016  & 47.0 & 3.0 \us & 6.2 \us\\
\calvin & 10,002  & 128.0 & 8.7 \us  & 10.8 \us\\
\end{tabular}
\vspace{-.05in}
\caption{{\em Leader's input consensus events per 10K requests, 8 threads.}
The ``\# Calls" column means the number of socket calls that went through \xxx
input consensus; ``Input" means average bytes of a server's inputs received in
these calls; ``SSD time" means the average time spent on storing these calls to
stable storage; and ``Quorum time" means the average time spent on waiting
quorum for these calls.}
\vspace{-.1in}
\label{tab:consensus-latency}
\end{table}

% Figure~\ref{fig:latency} shows \xxx's latency.
% % Bare consensus latency, micro events.

To deeply understand \xxx's performance overhead, we collected the number of
socket call events and consensus durations on the leader side.
Table~\ref{tab:consensus-latency} shows these statistics per 10K requests, 8
or max (if less than 8) threads. According to the consensus algorithm steps in
Figure~\ref{fig:consensus}, for each socket call, \xxx's leader does an ``L2":
SSD write (the ``SSD time" column in Table~\ref{tab:consensus-latency}) and an
``L4": quorum waiting phase (the ``quorum time" column). L4 implies backups'
performance because each backup stores the proposed socket call in local SSD and
then WRITEs a consensus reply to the leader.

By adding the last two columns in Table~\ref{tab:consensus-latency}, a 
\xxx input consensus took only 9.1 \us (\redis) to 22.4 \us (\mediatomb). 
% This consensus latency mainly depends on the ``Input" column: the average number of 
% data bytes received in socket calls (\eg, \mongodb has the largest received bytes).
\xxx's small consensus latency makes \xxx achieve reasonable throughputs 
in Figure~\ref{fig:tput} and response times Figure~\ref{fig:latency}.

% This small latency suggests that, even if clients are deployed within the same
% datacenter network, \xxx may still achieve acceptable overhead on many programs
% (although \xxx's deployment model is running server replicas in a datacenter and
% clients in LAN or WAN).

% \begin{table}[t]
% \footnotesize
% \centering
% % \vspace{-.2in}
% \begin{tabular}{lrr}
% {\bf Performance metric} & {\bf \zookeeper} & {\bf \xxx}\\
% \hline\\[-2.3ex]
% Throughput (requests/s) & 19,925   & 17,614 \\
% Consensus latency (\us) & 511.9  & 12.5\\
% \end{tabular}
% \vspace{-.1in}
% \caption{{\em Comparison with \calvin's \zookeeper replication.}}
% \vspace{-.2in}
% \label{tab:compare}
% \end{table}





% Comparison with Crane. Run Crane. TBD


% For seven nodes
% XXX TBD. We plan to buy more server machines for a larger scale of scalability
% evaluation.

% It
% would be interesting to see whether \xxx's scalability on replica group size is
% bounded by the outbound RDMA writes of RDMA NICs.



% Comparison with DARE. Three to five nodes.

\subsection{Checkpoint and Recovery} \label{sec:robust}

% \xxx's output checking protocol found different output results on \clamav,
% \openldap, and \mediatomb. The output divergence of \clamav was caused by its
% threading model: \clamav uses multiple threads to serve a directory scan
% request, where the threads scan files in parallel and append the scanned files
% into a shared output buffer protected by a mutex lock. Therefore, sometimes
% \clamav's output will diverge across replicas. Running a deterministic
% multithreading runtime with \clamav avoided this divergence~\cite{crane:sosp15}.
% \openldap and \mediatomb contained physical timestamps in their replies.
% \calvin did not send outputs to its client.

% TBD: what types of servers are good for output checking.

We ran the same performance benchmark as in \S\ref{sec:overhead} and measure 
programs' checkpoint timecost. Each \xxx periodic checkpoint operation 
(\S\ref{sec:checkpoint}) cost 0.12s to 11.6s on the evaluated server programs, 
depending on the amount of modified memory and files in the server programs 
since their own last checkpoint. \clamav incurred the largest checkpoint time 
(11.6s) because it loaded and scanned files in a \v{/lib} directory.

Checkpoint operations did not affect \xxx's performance in normal case because 
they were done on only one backup, and the leader and other backups can still 
reach consensus rapidly.
% This indicates \paxos's fault-tolerance strength: a 
% backup restart or failure does not affect consensus. 

To evaluate \xxx's \paxos recovery feature, we ran \xxx with \redis and 
manually kill one backup, and we did not observe a performance change in the 
benchmark runs. We then manually killed the \xxx leader and measured the 
latency of our RDMA-based leader election with three rounds 
(\S\ref{sec:election}). Figure~\ref{tab:election} shows \xxx's election latency 
from three to eleven replicas. Because \paxos leader election is rarely 
invoked in practice, although \xxx's election latency was slightly higher than 
its normal case consensus latency, we considered it reasonable.

% efficiency of \xxx's rocovery, we ran \clamav and tried to randomly trigger its
% output divergence. \clamav's rocovery time varied from 1.47s to 1.49s for
% backups and the leader, including extracting files in its local directory,
% restoring process state with CRIU, and reconnecting RDMA QPs with remote
% replicas.



% Heming: TBD, just estimation. Current 0.81s to 2.72s are from redis.





\begin{table}[h]
\footnotesize
\centering
\vspace{-.05in}
\begin{tabular}{lrrrrr}
{\bf \# Replicas} & {\bf 3} & {\bf 5} & {\bf 7} & {\bf 9} & {\bf 11}\\
\hline\\[-2.3ex]
Election latency (\us) & 10.7  & 12.0 & 12.8 & 13.5 & 14.0\\
\end{tabular}
\vspace{-.1in}
\caption{{\em \xxx's scalability on leader election.}}
\vspace{-.3in}
\label{tab:election}
\end{table}







% Once an output divergence is detected, \xxx will roll bacak the divergent
% replica, including leader and backups. For all the \nprog programs, our output
% checking protocol found these servers produced identical results except \clamav
% and \ssdb.
%
% \clamav's divergent output is caused by its special threading model:
% unlike the other evaluated programs which use one thread to server one client
% connection, \clamav uses multiple worker threads to serve a client request
% (\eg, scanning a directory path recursively). \xxx's worker threads
% concurrently contend for a global mutex lock and then append the scanned
% files into the output buffer for the client, causing a nondeterministic order
% of scanned files among all replicas. We manually compared \clamav's output
% across replicas, and we found the files were indeed the same except their order.
%
% We ran \ssdb on the same workload as in \S\ref{sec:overhead} and triggered a
% previous unkown concurrency bug in the QPUSH operations. This concurrency bug
% was triggered by concurrent client connections to push elements to a queue in
% \ssdb. In our evaluation, this bug was first triggered in a backup machine and
% caused a divergent output hash. the hashes of the other two replicas were
% still the same. \xxx's leader detected this divergence; it sent a
% rollback request to the divergent replica's guard, which took 1.2 \ms;
% the guard killed the \ssdb server and restored it from a prior
% checkpoint with 0.913 \ms, and the recovered replica reconnected to the other
% replicas and started to server requests in 0.09 \ms. Each process and file
% system checkpoint operation for \ssdb took 954 \ms.
%
% This minor divergence suggests that we hitted a bug, for two
% reasons. First, \xxx has enfored strongly consistent inputs across replicas, if
% only replicas diverge, this divergence must be caused by the server program
% itself, not different inputs. Second, only one replica diverged, this suggests
% that the output does not contain randome values (\eg, physical times, or the
% \clamav case), otherwise all replicas should diverged.
%
% We then carefully looked into the \ssdb source code and identified the bug. It
% was caused by incorrect synchronization to a global queue in \ssdb. We have
% reported this bug to the \ssdb developers with a suggested bug fixing
% patch~\cite{ssdb:bug}. Although \xxx does not intend to find bugs, this
% promising finding reflects that \xxx could be extended as an advanced testing
% tool: its fast, general SMR service lets it easily support general program, and
% its consistently enforce inputs and efficient output checking makes the minor
% replica output divergence a strong software bug indicator.
% Confidence. strongly consistent inputs.

%
% \subsection{Sensitivity of Parameters} \label{sec:sensitivity}

% Change output comparison periods. 1, 100, 1000, 10000. 1000 is the smallest
% number that starts to have negligible overhead. Run only with the server with
% largest recv() data size.

% Twait

% Tcomphash

% \subsection{Read-only Optimization} \label{sec:read-opt}

% %
