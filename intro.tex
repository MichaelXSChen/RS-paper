\section{Introduction} \label{sec:intro}

%P1: SMR difinition; traditional network message passing; reliable; attractive 
% for general servers. Agree-execute: must reach consensus and then execute a 
% request.
State machine replicaion (SMR) runs the same program on a 
number of replicas and uses a distributed consensus protocol (\eg, 
\paxos~\cite{crane:sosp15}) to enforce the same inputs among 
replicas. Consensus on a new input can be achieved as long as a majority 
of replicas agree, thus SMR can tolerate various faults such as minor replica 
failures. Attracted by this strong fault-tolerance, recently, several SMR 
systems~\cite{chubby:osdi, zookeeper, crane:sosp15, eve:osdi12, rex:eurosys14} 
have been built to greatly improve the availability of various server programs, 
because these programs tend to serve client requests at all time.

% P2: Performance too slow. Agree first and then execute. Even three nodes, one 
% round-trip (~400 us). Not for performance critical servers such as key-value.
% Batching: addressed throughput but not latency.
Unfortunately, despite these recent advances, SMR remains difficult to be 
widely adopted due to its high consensus latency. To agree on an input, 
traditional consensus protocols invoke at least one message round-trip between 
two replicas. Given that a \v{ping} in Ethernet takes hundreds of \us, a server 
program running in an SMR system with only three replicas must wait at least 
this time before processing an input. This latency may be acceptable for 
maintaining global configuration~\cite{chubby:osdi,zookeeper} or processing SQL 
transactions~\cite{crane:sosp15,eve:osdi12}, but prohibitive for 
key-value stores. To mitigate this challenge, some recent SMR 
systems~\cite{calvin:sigmod12,rex:eusorys14} batch requests into one 
consensus round. However, batching can only mitigate a server's throught lost; 
it may aggravate response time.
% As a possible consequence, although many recent storage 
% systems~\cite{drtm} 
% explicitly stated that they needed a replication system for high availability, 
% they finely didn't adopt the batching approach.

% P3: Another problem: scalability. As more nodes are in replica group, it is 
% getting much more slower to reach quorum. Event-driven to increase 
% parallilism, but still slow: despite the large latency, context switches (400 
% us).

As an SMR replica group grows, consensus protocols are even more difficult to 
scale, because a majority involves more replicas with more round-trips. One 
scalling approach (\eg, ~\cite{crane:sosp15}) may be using an event-driven 
model (\eg, Libevent~\cite{libevent}) to improve the parallilism of replicas' 
consensus round-trips. However, the high latency of a single round-trip still 
exists.
% and synchronization context switches (often takes hundreds of \us) in 
% the event loop of this model also adds latency.


% The second challenge is that an automated, fine-grained approach is needed to 
% avoid execution divergence of active (\ie, alive) replicas. Even in the absence 
% of replica failures or network partitions, the executions of different replicas 
% can still diverge due to contention of 
% inter-thread resources~\cite{coredet:asplos10} (\eg, shared memory) and systems 
% resources~\cite{racepro:sosp11} (\eg, files and network ports). This challenge 
% not only lies in standard SMR systems which require deterministic executions, 
% but it is also pervasive in commodity replication systems (\eg, \redis, 
% \memcached, and \mysql) that seek for fault-tolerance in some degree.

% P4.0: opportunity, RDMA. We argue that, network layers are not inherent.
Remote Direct Access Memory (RDMA) is promising on mititaging consensus latency 
because it becomes increasingly cheaper and pervasive in datacenters. RDMA 
allows a host machine to directly write to the memory of a remote host without 
involving the OS kernel or CPU at either host. For instance, a recent 
evaluation~\cite{pilaf:atc14} reports about 3 \us mean round-trip latency on 
RDMA write operations. However, despite much effort, it is still quite 
challenging to fully exploit the speed of RDMA in consensus protocols due to 
the unrichness of RDMA features. We explain why by elaborating two 
existing approaches below.

One straight forward approach XXX.

% P4: Falcon; key features. Hook sockets in servers.
This paper present \xxx, an SMR systems that replicates general server programs 
efficiently by exploiting the fastest RDMA operations. With \xxx, 
a server program just runs as if it is the single copy, and \xxx automatically 
deploys this program on replicas of machines. \xxx enforces same network 
inputs and verifies network outputs for replicas. If \xxx finds that a replica 
produces an different output from what other replicas agree on, \xxx recovers 
this replica to a previous program checkpoint and re-executes inputs that have 
been agreed on from the checkpoint. 


% First, introduce naive approach. IPoverIB.

% P5: Falcon: RDMA input coordination. Persistent stores; two RDMA writes 
% between two machines; no context switch.
To coordinate inputs among replicas, \xxx intercepts a server program's socket 
APIs (\eg, \recv) to caputure inputs and introduces a new RDMA-accelerated 
\paxos protocol to let replicas agree on these inputs. To ease understanding 
and checking tooks. this protocol complies with common style of popular paxos 
protocal~\cite{paxos:practical}. In the normal case of this protocol, contrast 
to existing implementations which require one network round-trip (\ie, two 
messages for every two replicas), our protocal only requires two most efficient 
one-sided write operations.

% P5.1: Support read-only optimization.

% P6: Falcon: output checker.
However, input coordination is not sufficient to practically enforce same 
execution states for the same program across replicas. Nowadays most server 
programs already adopt multi-threading or multi-process models to harness the 
power of multi-core and improve performance. Contentions on inter-thread 
resources (\eg, global memory and \pthread mutex locks) and systems resources 
(\eg, network ports and files) can easily cause program execution states among 
replicas to diverge an can never converge again. 

To address this challenge, recent SMR systems leverage either deterministic 
multithreading techniques~\cite{rex:eurosys14,crane:sosp15} or detecting 
divergence of execution by manually annotating program states by threads, 
artificially trading off performance or automatacity.

% Typical commodity 
% replication systems ignore this challenge and use `ping" to check whether 
% replicas are working as expected, but this coarse-grained approach can not 
% detect execution divergence of resource contentions because a program can just 
% compute wrong outputs without crashing. 

Our key idea is that we don't need to a program's every (or every batch) 
network outputs because they most replicas's outputs indicate that this output 
is most likely the produced one. Either necessary or sufficient. Not necessary 
because most executions already produce same program behaviors (including 
outputs) even with concurrency bugs. Not sufficient because it could be all 
replicas producing the same buggy output and bypass consensus protocols. All we 
need is just lazily compare outputs and if a divergence is detected, we roll 
back programs and re-execute them.

To implement this idea, \xxx's output verification protocol first . network 
outputs on each individual replica, computes  hash values incremental:
compute the hash value of a union of last hash value and the output  and 
periodically invoke our \paxos consensus protocol to exchange the hash value. 
Then, if minor replicas' outputs diverge from the majority ones, we just roll 
back and re-execute these minor replicas without perturbing the others to agree 
on and process new inputs. If a majority can not reach, \xxx simply rolls back 
the XXX (leader?). Evaluation confirmed that XX.XX\% cases.


% P7: conceptual level: complete architecture. agree-execute-enforcement.
In a conceptual level, to provide pratical SMR service for general programs, 
\xxx presents a new agree-execute-verify execution model, which contrasts from 
previous agree-execute models and execute-verify models. We argue that agree is 
essential to SMR due to its strong fault-tolerance on machine failures and 
packet losses (even RDMA networks have packet loss when machines fail or 
programs crash). Having a general input coordination protocol also mitigates 
the need of writing application-specific input mixer and manually code 
annotation. Moreover, a automatic, fast output verification protocol is 
essential to SMR because we aim to replicate general, diverse server programs 
that may diverge due to resource contentions. In sum, by coordinating inputs 
and verifying outputs among replicas, \xxx practically enforces same execution 
states and outputs among replicas.

% P8: implementation. POSIX. support checkpoint.
We implemented \xxx in Linux. \xxx intercepts common POSIX incoming socket 
operations (\eg, \accept and \recv) to coordinate inputs using the Infiniband 
RDMA architecture. \xxx also intercepts outcoming socket operations (\eg, 
\send) to invoke the output checking protocol. This simple, deployable 
interface design makes \xxx support general server programs without modifying 
them. To support practical checkpoint and restore on server 
programs, \xxx leverages \criu.

% P9: Evaluatuion, with highlight items, match abstract, but more details.
We evaluated \xxx on \nprog widely used or studied server programs, including 
\nkvprog key value stores (\redis, \memcached, \ssdb, and \mongodb), one SQL 
server \mysql, one anti-virus server \clamav, one multimedia storage server 
\mediatomb, one LDAP server \openldap, and one advanced transactional database 
\calvin (with \zookeeper as its SMR protocol). Our evaluation shows that

\begin{tightenum}
\item \xxx is general. For all evaluated programs, \xxx ran them without any 
modification except \calvin (we added a \nlinescalvin patch to make its client 
and server programs support sockets).

\item \xxx is fast. Compared to the \nprog servers' unreplicated executions, 
\xxx incurred merely \tputoverhead overhead on throughtput and \latencyoverhead 
on response time in average. \xxx is \fasterthanzookeeper faster than \calvin's 
zookeeper-based SMR service on response time.

\item \xxx is robust. Among XXX repeated executions, \xxx detected and 
recovered execution divergence caused by a software bug in \redis, while 
\redis's own replication service missed the bug.

\item \xxx is extensible. To extend optimization on read-only requests, XX 
lines of code in our two provided APIs, \xxx is able to avoid the read-only 
requests in \redis to do consensus and XX times faster than \redis's own 
replication system. 

\end{tightenum}  
% % tighten items, highlighted.

% P10: Conceptual contribution. Applications: other replications, parallel 
% program % analysis, and datacenter OS (it's efficiency and strong consistency 
% makes % system calls go beyond single machine).
% New design space. comprehensive model. many applications: other replications, 
% parallel analysis, datacenter OS.
Our major conceptual contribution is leveraging RDMA to make synchronized, 
\paxos-based replication adoptable. This new protocol incorporates fasted RDMA 
hardware features, while it still pertains same fault-tolerance guarantees as 
traditional \paxos protocols. \xxx has the potential to serve as an effective 
research template for other replication areas (\eg, byzantine fault-tolerance). 
In addition, a fast, general SMR service has been long persued as a fundamental 
building block for the emerging datacenter operation system.

% P10: Engineering contribution. Potential to substitue customized replication 
% in commodity systems and use a general ones. Easy to verify, easy to get 
% right, easy to use.
Our major engineering contribution includes the \xxx implemention and its 
evaluation on \nprog diverse, widely used server programs. Due to the lack of a 
general SMR system, industrial developers have spent tremondous efforts on 
building specific replication systems for their own programs and ``invent the 
wheels again and again". Note that understanding, building, and maintaining a 
usable SMR systems requires extreme expert knowledege, burdens, and are 
extremely challenging (so many \paxos papers). For example, the \redis or 
\memcached lag bug. Our \xxx system and evaluation has shown promising results 
on building a fast, general, and extendible SMR system and help developers 
greatly release these burdens. We have released all \xxx's source code, 
benchmarks, and raw evaluation results at \github.

% P11: Remaining of paper.
The remaining of this paper is organized as follows. \S\ref{sec:background} 
introduces background on \paxos and RDMA features. \S\ref{sec:overview} gives 
an overview of our \xxx system with key components and its applications in 
other areas. \S\ref{sec:input} presents our input coordination protocol. 
\S\ref{sec:output} output checking protocol. \S\ref{sec:limits} \xxx's 
discusses limitations. \S\ref{sec:evaluation} presents evaluation results, 
\S\ref{sec:related} discusses related work, and \S\ref{sec:conclusion} 
concludes.   