\section{Introduction} \label{sec:intro}

%P1: SMR difinition; traditional network message passing; reliable; attractive
% for general servers. Agree-execute: must reach consensus and then execute a
% request. Emphasis ordering services, Scatter, 8~12 nodes.

State machine replication (SMR) runs a program on several computer 
replicas and enforces same inputs for this program as long as a quorum 
majority of replicas still behave normally, tolerating various 
faults such as minor replicas failures and packet losses. To achieve 
this powerful 
fault-tolerance, SMR typically uses 
a distributed consensus protocol called 
\paxos~\cite{paxos:practical,paxos,paxos:simple,paxos:complex}. Recent SMR 
systems~\cite{eve:osdi12,rex:eurosys14,crane:sosp15} have greatly 
improved the availability of server programs that serve online requests and 
store important data.

% ~\cite{eve:osdi12,rex:eurosys14,crane:sosp15,ellis:thesis, 
% manos:hotdep10,scatter:sosp11,zookeeper, chubby:osdi}
% Furthermore, other recent 
% systems~\cite{tbd1,tbd2} extend \paxos to achieve byzantine 
% fault-tolerance~\cite{pbft:osdi99,zyzzyva:sosp07}.

% services~\cite{ellis:thesis,manos:hotdep10,scatter:sosp11},
% leader election~\cite{zookeeper, chubby:osdi}, and
% fault-tolerance~\cite{eve:osdi12,rex:eurosys14,crane:sosp15}. A \paxos protocol
% runs the same program on a group of replicas and enforces a strongly
% consistent order of inputs for this program, as long as a quorum (typically,
% majority) of replicas still behave correctly.

% TBD: Scatter description is not very clear; on replica sub key range part.
% Due to this strong fault-tolerance, \paxos is widely served in many systems.
% For instance, Scatter~\cite{scatter:sosp11} runs 8$\sim$12 replicas in each
% \paxos group to order client requests, and it lets replicas respond requests
% in parallel. A bigger group size will improve Scatter throughput. 





% Typically, \paxos assigns a replica as the leader to propose
% consensus requests, and the other replicas agree or reject requests.
% An input consensus can achieve as long as a majority of replicas
% agree, thus SMR can tolerate various faults such as minor replica failures.


% P2: Performance too slow. Agree first and then execute. Even three nodes, one
% round-trip (~400 us). Not for performance critical servers such as key-value.
% Batching: addressed throughput but not latency.
Unfortunately, existing \paxos protocols incur prohibitive performance overhead 
on server programs. For efficiency, \paxos typically assigns one replica as the 
leader to invoke consensus requests, and the other replicas as backups to agree 
on requests. To agree on an input, at least one message round-trip is required 
between the leader and a backup. A round-trip causes big latency as it goes 
through TCP/IP layers such as OS kernel. This latency may be acceptable for 
leader election~\cite{chubby:osdi,zookeeper} or heavyweight 
transactions~\cite{crane:sosp15,eve:osdi12}, but undesirable for
key-value servers (\eg, \redis and \memcached).
% To address this challenge,
% some systems~\cite{calvin:sigmod12,rex:eurosys14} batch requests into one
% consensus round. However, batching will only mitigate throughput lost and it
% will aggravate request latency.
% As a possible consequence, although many recent storage
% systems~\cite{drtm}
% explicitly stated that they needed a replication system for high availability,
% they finely didn't adopt the batching approach.

% P3: Another problem: scalability. As more nodes are in replica group, it is
% getting much more slower to reach quorum. Event-driven to increase
% parallilism, but still slow: despite the large latency, context switches (400
% us).

As the number of concurrent requests or replicas increases, \paxos consensus 
latency often increases linearly~\cite{scatter:sosp11} due to the linearly 
increasing number of consensus messages. This limited scalability prevents SMR 
from deploying with general server programs for two reasons. First, the 
performance of server porgrams are often scalable to number of CPU cores on 
serving concurent requests. Second, some advanced SMR systems (\eg, 
Azure~\cite{azure:book}) often deploy seven or nine replicas to handle replica 
failures and updates.

To improve scalability, one approach 
is introducing parallel techniques such as multithreading~\cite{zookeeper, 
spaxos:srds12} or asynchronous IO~\cite{crane:sosp15, libpaxos}. However, the 
high TCP/IP round-trip latency still exists, and synchronizations in these 
techniques frequently invoke expensive OS events such as context switches. We 
ran four \paxos-like protocols~\cite{zookeeper, spaxos:srds12, crane:sosp15, 
libpaxos} on 24 cocurrent consensus requests on a computer with 24 CPU cores 
and 40Gbps network. We found that, when replica group size increased from 3 to 
9, the consensus latency of 
three protocols increased by \tradlatencyincreaselow to 
\tradlatencyincreasehigh, and \systemcostlow to \systemcosthigh of the increase 
was in OS kernel.

Another scalability approach is maintaining multiple instances of \paxos, 
including batching requests~\cite{calvin:sigmod12}, partitioning program 
states~\cite{scatter:sosp11,ssmr:dsn14,ellis:thesis}, splitting consensus 
leadership~\cite{mencius:osdi08,spaxos:srds12}, and hierarchical 
replication architecture~\cite{manos:hotdep10,scatter:sosp11}. 
These systems have improved throughput. However, the core of these systems, 
\paxos, still has an unscalable consensus latency. Previous
work~\cite{scatter:sosp11,ellis:thesis,manos:hotdep10} explicitly desires to 
have a \paxos protocol with a more scalable consensus latency.

%  advanced
% replication
% models are proposed, including
% multi-leader~\cite{epaxos:sosp13,mencius:osdi08},
% cluster~\cite{manos:hotdep10}, and nested consensus
% models~\cite{scatter:sosp11}.

% One
% scalling approach (\eg, ~\cite{crane:sosp15}) may be using an event-driven
% model (\eg, Libevent~\cite{libevent}) to improve the parallilism of replicas'
% consensus round-trips. However, the high latency of a single round-trip still
% exists.
% and synchronization context switches (often takes hundreds of \us) in
% the event loop of this model also adds latency.


% The second challenge is that an automated, fine-grained approach is needed to
% avoid execution divergence of active (\ie, alive) replicas. Even in the absence
% of replica failures or network partitions, the executions of different replicas
% can still diverge due to contention of
% inter-thread resources~\cite{coredet:asplos10} (\eg, shared memory) and systems
% resources~\cite{racepro:sosp11} (\eg, files and network ports). This challenge
% not only lies in standard SMR systems which require deterministic executions,
% but it is also pervasive in commodity replication systems (\eg, \redis,
% \memcached, and \mysql) that seek for fault-tolerance in some degree.

% P4.0: opportunity, RDMA. We argue that, network layers are not inherent.
% Memory semantic.
Fortunately, Remote Direct Memory Access (RDMA) becomes a possible scalability 
solution as it becomes common in datacenters. RDMA not only bypasses OS 
kernel, but it also provides dedicated network hardware to achieve a fast 
round-trip. For instance, the fastest RDMA operation allows a process to 
directly write to a remote replica's process without involving the remote OS 
kernel or CPU (``one-sided" operation). To ensure an RDMA write successfully 
resides in the remote memory, local process polls an RDMA ACK from remote 
NIC. Such a RDMA round-trip takes only about 3 \us~\cite{pilaf:usenix14}.


% A strawman approach: DARE. RDMA communication primitives themselves have
% scalability issues.
% DARE weaken memory semantic. Need to come up with a reason.
However, due to the unrich RDMA operation types, fully exploiting RDMA
speed in software systems is widely considered 
challenging by the community~\cite{pilaf:usenix14,herd:sigcomm14,
farm:sosp15,dare:hpdc15}. For instance, \dare~\cite{dare:hpdc15} presents a
two-round, RDMA-based \paxos protocol in a sole-leader manner: leader does all 
RDMA workloads and backups do nothing. \dare was fast with 3 or 5 
replicas. However, our evaluation shows that, as replica group grows, the 
leader met scalability bottlenecks (\eg, polling ACKs). \dare's consensus 
latency increased by \darescalability as the group grows by 35x
(\S\ref{sec:eval-dare}).

% For
% instance, one-sided RDMA operations eliminate remote replicas' participations,
% but traditional \paxos protocols require non-leader replicas to examine the
% leader's consensus requests.

% First, the leader uses RDMA to write the consensus requests to all replicas and
% polls RDMA ACKs to check whether the writes succeed. Second, for the successful
% writes, the leader does another round of RDMA writes to mark the writes as
% successful on other replicas, and poll ACKs on these writes. One a majority of
% successful writes in the second round, DARE reaches a consensus.

% Our key idea: pure remote-memory consensus. A fully scalable RDMA Paxos
% should allow leader to process requests pure on memory. Leader and replica
% % join % consensus; have stable storage with this benefit.
% Why is this idea fast. Mulithreading.
% We: heavily exploiting memory semantic of RDMA. Use more CPUs; save ACKs.
Our key observation is that we should carefully separate RDMA workloads among
the leader and backups, especially in a scalability-sensitive context. 
Intuitively, we can let both leader and backups do RDMA writes directly on 
destination replicas' memory, and let all replicas poll their local memory to 
receive messages.

Although doing so will consume more CPU resources than a sole-leader 
protocol~\cite{dare:hpdc15}, it has three major benefits. First, the leader 
has less workloads. Second, both leader and backups participate in consensus, 
which makes it possible to reach consensus with only one 
round~\cite{paxos:practical}. Third, all replicas can get rid of 
the expensive RDMA ACK polling and just receive consensus messages on their 
bare, local memory. An analogy is threads receiving other threads' data via 
bare memory, a fast and scalable computation pattern.


% An analogy is that threads receive other threads' data and
% signals via bare memory, a fast and scalable multithreading pattern. Now the
% only RDMA primitive our \paxos replicas involve is just sending RDMA writes
% (\ie, copying the data to be sent to NIC). Our evaluation showed that
% most of such outbound RDMA write operations took less than 0.2 \us, much faster
% faster than inbound RDMA operations.

% Why is this idea feasible. Paxos can already handle unreliability.
% Tech challenge? data integrity. Storage. Checkpoints? Others?
This observation may raise reliability concerns because now the leader has no 
clue on whether remote writes succeed. Fortunately, \paxos's protocol 
already tolerates various reliability issues, including message losses caused by
hardware or program failures.

% Now one just needs a runtime system to 
% efficiently ensure the atomicity of remote writes among replicas 
% (\S\ref{sec:normal}).



% As a common RDMA practice, to ensure that such a write
% successfully resides in the memory of a remote process, the local process
% should wait until the remote NIC (network interface card) sends an ACK to the
% local host's NIC. An evaluation~\cite{pilaf:usenix14} shows that such a write
% round-trip takes only $\sim$3 \us in the Infiniband networking
% architecture~\cite{infiniband}.

% However, it is technically challenging to fully exploit RDMA speed in \paxos
% protocols due to the unrichness of RDMA features. We present this challenge in
% detail by elaborating two possible approaches below. One straightforward
% approach is IP over Infiniband (IPoIB). This approach emulates TCP/IP on RDMA
% hardware so that traditional \paxos implementations can enjoy RDMA speedup
% without modifications. However, this loose combination of RDMA and \paxos is
% still one order of magnitude slower than fastest RDMA operations because IPoIB
% goes through the OS kernel and copies network data between kernel and user
% space.

% To the best of our knowledege, DARE's approach achieves the fastest consensus
% speed in existing approaches because all communications are simply replaced
% with the fatest RDMA writes (although we argue that a stable storage for
% consensus requests should be added to ensure \paxos durability).

% However, this
% approach faces a scalability challenge: to ensure a remote replica is alive,
% each step has to wait ACKs from the previous step before it starts, and each
% RDMA write has to wait for its own ACK. In this pure leader-based algorithm,
% ACKs are necessary for every next step to start. As the replica group size
% grows, the leader has to do RDMA writes to remote replicas one by one, making
% its consensus latency grows linearly to replica group size (confirmed in our
% evaluation).
%
% to address this scalability challenge is that
% simply replacing RDMA writes with \paxos communications is not sufficient, and
% In addition to mitigating consensus latency, RDMA creates
% new opportunity to address the \paxos scalability problem, because we
% can integrate RDMA features \emph{tightly} within the fault-tolerant nature of
% \paxos. In essence, \paxos already tolerates various faults, including
% machine failures and process crashes. Therefore, we can safely ignore the ACKs
% in RDMA writes and let \paxos handle the (un)reliability of these writes.
%
% This integration of \paxos and RDMA features looks simple, but it leads to
% a fast, scalable \paxos consensus algorithm with three steps. First, the leader
% stores a consensus request in local stable storage. Second, it does RDMA writes
% in parallel to put this request to the memory of remote replicas without
% waiting any RDMA ACKs. Remote replicas also work in parallel: they poll from
% their local memory, store this request in local storage, and send consensus
% replies to the leader with RDMA writes, without waiting any RDMA ACKs
% either. Third, once the leader sees a majority of replies in local memory,
% a consensus is reached.
%
% In the second step of this algorithm, both the leader and remote replicas work
% in parallel, thus a complete consensus latency approximately consists of
% three operations: a leader's write to stable storage, a remote replica's write
% to local storage, and a RDMA write round-trip. This consensus
% latency is no longer firmly correlated with replica group size (confirmed in
% our evaluation); its scalability is now mainly bounded by the capacity of
% outbound RDMA writes in the NIC hardware. By making the core of \paxos
% scalable, other advanced replication
% models~\cite{epaxos:sosp13,mencius:osdi08,scatter:sosp11,manos:hotdep10} can
% scale even better.
%  (currently, 16~\cite{herd:sigcomm})

% P4: Falcon; key features. Hook sockets in servers.
We present \xxx,\footnote{We name our system after
falcon, one of the fastest birds.} a new RDMA-based \paxos protocol and its
runtime system. In \xxx, all replicas directly write to destination
replicas' memory and poll messages from local memory to receive messages, and 
our runtime system handles other technical challenges such as message 
atomicity (\S\ref{sec:normal}), efficient input logging (\S\ref{sec:logging}), 
and failure recovery (\S\ref{sec:checkpoint}).

Similar to general SMR systems~\cite{rex:eurosys14,crane:sosp15},
\xxx's design supports general, unmodified server programs. Within \xxx, a
program runs as is. \xxx automatically deploys this program on replicas, 
intercepts inputs from a server program's inbound socket calls (\eg, \recv), and 
invokes its \paxos protocol to enforce same inputs across replicas.



% \xxx then provides an optional
% rollback/restore mechanism to make an effort to restore the diverged replicas.
% Because hash computation is efficient and output consensus is invoked rarely,
% this output checking protocol introduced negligible performance impact in our
% evaluation.

% In a consensus protocol level, \xxx carefully tackles several technical
% challenges, including handling an atomicity challenge (\S\ref{sec:normal}) and
% concurrent connections (\S\ref{sec:concurrent}).

% If \xxx finds that
% a replica produces an different output from what other replicas agree on, \xxx
% recovers this replica to a previous program checkpoint and re-executes inputs
% that have been agreed on from the checkpoint.

% P6: Falcon: output checker.
% However, to practically replicate general server programs, only enforcing same
% inputs is often not enough. An automated, efficient output checking mechanism
% that can improve the assurance on ``replicas run in sync" is still missing in
% existing SMR
% systems~\cite{calvin:sigmod12,rex:eurosys14,crane:sosp15,dare:hpdc15}.
% Most server programs use multithreading to harness the power of multi-core
% hardware. Nondeterminism~\cite{racepro:sosp11,dmp:asplos09,coredet:asplos10,
% cui:tern:osdi10, kendo:asplos09,
% dthreads:sosp11,peregrine:sosp11,parrot:sosp13,determinator:osdi10} caused by
% contentions in inter-thread resources (\eg, global memory and locks) and systems
% resources (\eg, network ports) can easily cause program execution states to
% diverge across replicas and compute wrong outputs to clients.
%
% To tackle nondeterminism, SMR systems either use deterministic multithreading
% and replay approaches~\cite{rex:eurosys14,crane:sosp15,ddos:asplos13}, or they
% rely on manually annotating share states in program code to detect execution
% divergence~\cite{eve:osdi12}. These approaches fall short in performance
% or automation.



% This
% protocol automatically, efficiently checks the fine-grained network outputs
% and improves assurance on whether replicas run in sync.

% A practical
% output checking mechanism is missing in widely deployed replication
% systems (\eg, \redis and \mysql) either, although these replication sytems
% provide weaker fault-tolerance or consistency guarantees than SMR for better
% performance.
%
% Two approaches for checking whether replicas run in sync exists. Existing
% widely deployed systems typically use \v{ping} to check whether replicas run in
% sync, but this coarse-grained checking will miss output divergence caused by
% tricky concurrency bugs~\cite{lu:concurrency-bugs}. Eve

% First, introduce naive approach. IPoverIB.

% P5: Falcon: RDMA input coordination. Persistent stores; two RDMA writes
% between two machines; no context switch.
% To coordinate inputs among replicas, \xxx intercepts a server program's socket
% APIs (\eg, \recv) to caputure inputs and introduces a new RDMA-accelerated
% \paxos protocol to let replicas agree on these inputs. To ease understanding
% and checking tooks. this protocol complies with common style of popular paxos
% protocal~\cite{paxos:practical}. In the normal case of this protocol, contrast
% to existing implementations which require one network round-trip (\ie, two
% messages for every two replicas), our protocal only requires two most efficient
% one-sided write operations.

% P5.1: Support read-only optimization.



% To address this challenge, recent SMR systems leverage either deterministic
% multithreading techniques~\cite{rex:eurosys14,crane:sosp15} or detecting
% divergence of execution by manually annotating program states by threads,
% artificially trading off performance or automatacity.

% Typical commodity
% replication systems ignore this challenge and use `ping" to check whether
% replicas are working as expected, but this coarse-grained approach can not
% detect execution divergence of resource contentions because a program can just
% compute wrong outputs without crashing.

% Our key idea is that we don't need to a program's every (or every batch)
% network outputs because they most replicas's outputs indicate that this output
% is most likely the produced one. Either necessary or sufficient. Not necessary
% because most executions already produce same program behaviors (including
% outputs) even with concurrency bugs. Not sufficient because it could be all
% replicas producing the same buggy output and bypass consensus protocols. All we
% need is just lazily compare outputs and if a divergence is detected, we roll
% back programs and re-execute them.

% To implement this idea, \xxx's output verification protocol first . network
% outputs on each individual replica, computes  hash values incremental:
% compute the hash value of a union of last hash value and the output  and
% periodically invoke our \paxos consensus protocol to exchange the hash value.
% Then, if minor replicas' outputs diverge from the majority ones, we just roll
% back and re-execute these minor replicas without perturbing the others to agree
% on and process new inputs. If a majority can not reach, \xxx simply rolls back
% the XXX (leader?). Evaluation confirmed that XX.XX\% cases.


% P7: conceptual level: complete architecture. agree-execute-enforcement.
% In a conceptual level, to provide pratical SMR service for general programs,
% \xxx presents a new agree-execute-verify execution model, which contrasts from
% previous agree-execute models and execute-verify models. We argue that agree is
% essential to SMR due to its strong fault-tolerance on machine failures and
% packet losses (even RDMA networks have packet loss when machines fail or
% programs crash). Having a general input coordination protocol also mitigates
% the need of writing application-specific input mixer and manually code
% annotation. Moreover, a automatic, fast output verification protocol is
% essential to SMR because we aim to replicate general, diverse server programs
% that may diverge due to resource contentions. In sum, by coordinating inputs
% and verifying outputs among replicas, \xxx practically enforces same execution
% states and outputs among replicas.

% P8: implementation. POSIX. support checkpoint.
We implemented \xxx in Linux. \xxx intercepts POSIX inbound socket calls
(\eg, \accept and \recv) to coordinate inputs using the Infiniband
RDMA architecture~\cite{infiniband}. To help people inspect whether replicas 
run in sync, on top of \xxx's \paxos protocol, \xxx provides an efficient 
network output checking protocol. To recover or add replicas, \xxx 
leverages \criu~\cite{criu} to automatically, efficiently checkpoint a program 
without perturbing consensus in normal case.
% on one backup replica.
% , so it does not affect other replicas to reach consensus rapidly.

% \spaxos
% is designed to achieve scalable throughput when more replicas are added. 

% P9: Evaluatuion, with highlight items, match abstract, but more details.
We compared \xxx with five popular, open source \paxos-like implementations,
including four traditional ones (\libpaxos~\cite{libpaxos},
\zookeeper~\cite{zookeeper}, \crane~\cite{crane:sosp15} and
\spaxos~\cite{spaxos:srds12}), and an RDMA-based one 
(\dare~\cite{dare:hpdc15}). We evaluated \xxx on \nprog widely used or studied 
server programs, including
\nkvprog key-value stores (\redis~\cite{redis}, \memcached~\cite{memcached},
\ssdb~\cite{ssdb}, and \mongodb~\cite{mongodb}), a SQL server
\mysql~\cite{mysql}, an anti-virus server \clamav~\cite{clamav}, a multimedia
server \mediatomb~\cite{mediatomb}, an LDAP server \openldap~\cite{openldap}, 
and a transactional database \calvin~\cite{calvin:sigmod12}. Evaluation shows 
that

\begin{tightenum}
\item \xxx achieves one order of magnitude higher scalability and one
order of magnitude faster latency than the literature 
(Figure~\ref{fig:scalability}). \xxx's consensus latency outperforms 4 
popular \paxos protocols by \comptradlow to \comptradhigh on 3 to 
9 replicas. \xxx is faster than \dare by up to \fasterDARE. When changing the 
replica group size from 3 to 105 (a 35x
increase), \xxx's consensus latency increases merely from \xxxlatencythree \us 
to \xxxlatencyonezerofive \us (a \xxxscalability, sub-linear increase).

\item \xxx is general and easy to use. For all \nprog evaluated programs, \xxx 
ran them without any modification except \calvin.

%  (we added a \nlinescalvin-line patch to make
% \calvin's client and server communicate with sockets)

\item \xxx incurs low overhead on all \nprog evaluated server programs.
With 9 replicas, compared to servers' own unreplicated executions, \xxx
incurred only \tputoverhead overhead on throughput and \latencyoverhead on
response time in average.

\item \xxx is robust. On leader failures, its leader election
latency was reasonable and scalable.

% \xxx's consensus latency is \fasterthanzookeeper
% faster than \calvin's SMR service \zookeeper.





% \item \xxx is extensible. To extend optimization on read-only requests, XX
% lines of code in our two provided APIs, \xxx is able to avoid the read-only
% requests in \redis to do consensus and XX times faster than \redis's own
% replication system.

\end{tightenum}
% % tighten items, highlighted.

% P10: Conceptual contribution. Applications: other replications, parallel
% program % analysis, and datacenter OS (it's efficiency and strong consistency
% makes % system calls go beyond single machine).
% New design space. comprehensive model. many applications: other replications,
% parallel analysis, datacenter OS.
Our major contribution is a new RDMA-based \paxos protocol that achieves 
low consensus latency on over 100 replicas. \xxx has the potential to largely 
improve both the scale and performance of many replication 
systems~\cite{scatter:sosp11,manos:hotdep10,crane:sosp15,rex:eurosys14, 
ssmr:dsn14,spaxos:srds12}. For instance, Scatter~\cite{scatter:sosp11} 
previously deploys 8$\sim$12 replicas in each \paxos group, and now it can 
deploy hundreds of replicas in each group and achieves much better performance. 
Overall, a fast, scalable, and general service, \xxx may significantly promote 
the deployments of \paxos and improve both the consistency and 
fault-tolerance of various systems in datacenters.

% Many \paxos-based replication 
% systems~\cite{scatter:sosp11,ssmr:dsn14,mencius:osdi08,spaxos:srds12,
% manos:hotdep10,scatter:sosp11} can scale even better with \xxx.

% \xxx
% can also be applied to broad areas, including other replication protocols (\eg,
% byzantine fault-tolerance~\cite{zyzzyva:sosp07,pbft:osdi99}), distributed
% program analyses, and future datacenter operating systems (\S\ref{sec:apps}).
% All \xxx source code, benchmarks, and evaluation results are
% available at \github.
% In addition, a fast, general SMR service has been long persued as a fundamental
% building block for the emerging datacenter operation system.

% P10: Engineering contribution. Potential to substitue customized replication
% in commodity systems and use a general ones. Easy to verify, easy to get
% right, easy to use.
% Our major engineering contribution includes the \xxx implemention and its
% evaluation on \nprog diverse, widely used server programs. Due to the lack of a
% general SMR system, industrial developers have spent tremondous efforts on
% building specific replication systems for their own programs and ``invent the
% wheels again and again". Note that understanding, building, and maintaining a
% usable SMR systems requires extreme expert knowledege, burdens, and are
% extremely challenging (so many \paxos papers). For example, the \redis or
% \memcached lag bug. Our \xxx system and evaluation has shown promising results
% on building a fast, general, and extendible SMR system and help developers
% greatly release these burdens. We have released all \xxx's source code,
% benchmarks, and raw evaluation results at \github.

% P11: Remaining of paper.
% \xxx is deployable: all source code, benchmarks, and raw evaluation results are 
% available at \github. 
The remaining of this paper is organized as follows.
\S\ref{sec:background} introduces the background of \paxos and RDMA.
\S\ref{sec:overview} gives an overview of \xxx. \S\ref{sec:input} presents 
\xxx's consensus protocol with its runtime system. \S\ref{sec:output} describes 
the network output checking protocol. \S\ref{sec:impl} describes implementation 
details. \S\ref{sec:discuss} compares \dare with \xxx and discusses \xxx's 
current limitations. \S\ref{sec:evaluation} presents evaluation results, 
\S\ref{sec:related} discusses related work, and \S\ref{sec:conclusion} 
concludes. 

\begin{figure}[t]
\centering
\vspace{-.10in}
\includegraphics[width=.4\textwidth]{figures/traditional_paxos_latency}
\vspace{-.15in}
\caption{{\em Consensus latency of six \paxos-like protocols.} All protocols 
ran with 20 concurrent proposing clients. Both X and Y axes are broken to fit 
in all these protocols.}
\label{fig:scalability}
\vspace{-.20in}
\end{figure}
