\section{Introduction} \label{sec:intro}

%P1: SMR difinition; traditional network message passing; reliable; attractive 
% for general servers. Emphasis persistent storage.
% Define ``synchronized replication": store to stable storage, must reach 
% consensus and then execute a request.
State machine replicaion (SMR) runs the same program on a 
number of replicas and uses a distributed consensus protocol (\eg, 
\paxos~\cite{crane:sosp15}) to enforce the same inputs among 
replicas. Consensus on a new input can be achieved if a majority of 
replicas agree, thus SMR can tolerate various faults such as minor replica 
failures. SMR's strong fault-tolerance mainly stems from its 
\emph{synchronized replication}: before processing an input, each replica must 
stores this input in local stage storage and receives a majority of replica 
aggreements. Due to this strong fault-tolerance, recent SMR systems have shown 
promising results on greatly improving the availability of various programs, 
including systems that target both specific programs~\cite{chubby:osdi, 
zookeeper} and general programs~\cite{crane:sosp15,eve:osdi12,rex:eurosys14}.

% P2: most servers lack such a service due to two challenges. First, two slow, 
% TCP/IP. Example, Calvin. For this reason, many storage systems give in SMR 
% (\eg, % Haibo's), although themselves really want SMR.
Unfortunately, despite these promising advances, two practical challenges still 
prevent SMR from being widely deployed for general programs, especially 
server programs that naturally demand high availability. The first challenge is 
performance. To reach consensus on input requests, traditional consensus 
protocols invoke TCP/UDP messages, which go through software network layers 
and OS kernels and often cause prohibitive latency. This network latency is 
especially severe in modern server storage servers (\eg, key-value stores) that 
tend to move computation and data into memory. To mitigate this challenge, Some 
SMR systems~\cite{calvin:sigmod12,rex:eusorys14} batch requests into one 
consensus round, but this could only mitigate throught lost but not latency. As 
a possible consequence, although many recent storage systems~\cite{drtm} 
explicitly stated that they needed a replication system for high availability, 
they finely didn't adopt the batching approach.

% P3: no systematic mechanism to automatically enforce execution states for 
% these servers. Current approach: (1) ping. (2) DMT, record replay. (3) 
% verification. Lack confidence on SMR.
The second challenge is that an automated, fine-grained approach is needed to 
avoid execution divergence of active (\ie, alive) replicas. Even in the absence 
of replica failures or network partitions, the executions of different replicas 
can still diverge due to contention of 
inter-thread resources~\cite{coredet:asplos10} (\eg, shared memory) and systems 
resources~\cite{racepro:sosp11} (\eg, files and network ports). This challenge 
not only lies in standard SMR systems which require deterministic executions, 
but it is also pervasive in commodity replication systems (\eg, \redis, 
\memcached, and \mysql) that seek for fault-tolerance in some degree.

% P4.0: opportunity, RDMA. We argue that, network layers are not inherent.
Recently, Remote Direct Access Memory (RDMA) has become increasingly 
cost-efficient to be adopted in data center networking. RDMA's advantages such 
as make opens new opportunity to build fast consensus protocols. For 
instance, one-sided write operations allow one machine to directly writes to 
a remote machine without the involement of remote machine's software network 
layers, OS kernels, or CPU. Although these software layers play important roles 
in the reliability of traditional network communications, we argue that these 
layers are not inherent to distributed consensus protocols. For 
instance, the \paxos consensus protocol has theoretically proven in the 
tolerance of of various faults (\eg, NIC failures and OS kernel crashes) and 
provided high availability in practice.

% P4: Falcon; key features. Hook sockets in servers.
This paper present \xxx, an SMR systems that replicates general server programs 
efficiently by exploiting the fastest RDMA operations. With \xxx, 
a server program just runs as if it is the single copy, and \xxx automatically 
deploys this program on replicas of machines. \xxx enforces same network 
inputs and verifies network outputs for replicas. If \xxx finds that a replica 
produces an different output from what other replicas agree on, \xxx recovers 
this replica to a previous program checkpoint and re-executes inputs that have 
been agreed on from the checkpoint. 


% First, introduce naive approach. IPoverIB.

% P5: Falcon: RDMA input coordination. Persistent stores; two RDMA writes 
% between two machines; no context switch.
To coordinate inputs among replicas, \xxx intercepts a server program's socket 
APIs (\eg, \recv) to caputure inputs and introduces a new RDMA-accelerated 
\paxos protocol to let replicas agree on these inputs. To ease understanding 
and checking tooks. this protocol complies with common style of popular paxos 
protocal~\cite{paxos:practical}. In the normal case of this protocol, contrast 
to existing implementations which require one network round-trip (\ie, two 
messages for every two replicas), our protocal only requires two most efficient 
one-sided write operations.

% P5.1: Support read-only optimization.

% P6: Falcon: output checker.
However, input coordination is not sufficient to practically enforce same 
execution states for the same program across replicas. Nowadays most server 
programs already adopt multi-threading or multi-process models to harness the 
power of multi-core and improve performance. Contentions on inter-thread 
resources (\eg, global memory and \pthread mutex locks) and systems resources 
(\eg, network ports and files) can easily cause program execution states among 
replicas to diverge an can never converge again. 

To address this challenge, recent SMR systems leverage either deterministic 
multithreading techniques~\cite{rex:eurosys14,crane:sosp15} or detecting 
divergence of execution by manually annotating program states by threads, 
artificially trading off performance or automatacity.

% Typical commodity 
% replication systems ignore this challenge and use `ping" to check whether 
% replicas are working as expected, but this coarse-grained approach can not 
% detect execution divergence of resource contentions because a program can just 
% compute wrong outputs without crashing. 

Our key idea is that we don't need to a program's every (or every batch) 
network outputs because they most replicas's outputs indicate that this output 
is most likely the produced one. Either necessary or sufficient. Not necessary 
because most executions already produce same program behaviors (including 
outputs) even with concurrency bugs. Not sufficient because it could be all 
replicas producing the same buggy output and bypass consensus protocols. All we 
need is just lazily compare outputs and if a divergence is detected, we roll 
back programs and re-execute them.

To implement this idea, \xxx's output verification protocol first . network 
outputs on each individual replica, computes  hash values incremental:
compute the hash value of a union of last hash value and the output  and 
periodically invoke our \paxos consensus protocol to exchange the hash value. 
Then, if minor replicas' outputs diverge from the majority ones, we just roll 
back and re-execute these minor replicas without perturbing the others to agree 
on and process new inputs. If a majority can not reach, \xxx simply rolls back 
the XXX (leader?). Evaluation confirmed that XX.XX\% cases.


% P7: conceptual level: complete architecture. agree-execute-enforcement.
In a conceptual level, to provide pratical SMR service for general programs, 
\xxx presents a new agree-execute-verify execution model, which contrasts from 
previous agree-execute models and execute-verify models. We argue that agree is 
essential to SMR due to its strong fault-tolerance on machine failures and 
packet losses (even RDMA networks have packet loss when machines fail or 
programs crash). Having a general input coordination protocol also mitigates 
the need of writing application-specific input mixer and manually code 
annotation. Moreover, a automatic, fast output verification protocol is 
essential to SMR because we aim to replicate general, diverse server programs 
that may diverge due to resource contentions. In sum, by coordinating inputs 
and verifying outputs among replicas, \xxx practically enforces same execution 
states and outputs among replicas.

% P8: implementation. POSIX. support checkpoint.
We implemented \xxx in Linux. \xxx intercepts common POSIX incoming socket 
operations (\eg, \accept and \recv) to coordinate inputs using the Infiniband 
RDMA architecture. \xxx also intercepts outcoming socket operations (\eg, 
\send) to invoke the output checking protocol. This simple, deployable 
interface design makes \xxx support general server programs without modifying 
them. To support practical checkpoint and restore on server 
programs, \xxx leverages \criu.

% P9: Evaluatuion, with highlight items, match abstract, but more details.
We evaluated \xxx on \nprog server programs, including \npopularprog 
widely used server programs: \nkvprog key value stores (\redis, 
\memcached, \ssdb, and \mongodb), one SQL server \mysql, one anti-virus server 
\clamav, one multimedia storage server \mediatomb, one LDAP server \openldap, 
one FTP server \tftp. We have also evaluated \calvin, an advanced transactional 
database system that leverages \zookeeper as its SMR service. Our evaluation 
shows that

\begin{tightenum}
\item \xxx is general. For the \npopularprog widely used server programs, \xxx 
ran them without any modification. We only needed to modify \calvin because it 
integrates its sever and client in the same process; we wrote a \nlinescalvin 
patch to make \calvin's server use POSIX sockets to accept client requests.

\item \xxx is fast. Compared to the \nprog servers' unreplicated executions, 
\xxx incurred merely \tputoverhead overhead on throughtput and \latencyoverhead 
on response time in average. \xxx is \fasterthanzookeeper faster than \calvin's 
zookeeper-based SMR service on response time.

\item \xxx is robust. Among XXX repeated executions, \xxx detected and 
recovered execution divergence caused by a software bug in \redis, while 
\redis's own replication service missed the bug.

\item \xxx is extensible. To extend optimization on read-only requests, XX 
lines of code in our two provided APIs, \xxx is able to avoid the read-only 
requests in \redis to do consensus and XX times faster than \redis's own 
replication system. 

\end{tightenum}  
% % tighten items, highlighted.

% P10: Conceptual contribution. Applications: other replications, parallel 
% program % analysis, and datacenter OS (it's efficiency and strong consistency 
% makes % system calls go beyond single machine).
% New design space. comprehensive model. many applications: other replications, 
% parallel analysis, datacenter OS.
Our major conceptual contribution is leveraging RDMA to make synchronized, 
\paxos-based replication adoptable. This new protocol incorporates fasted RDMA 
hardware features, while it still pertains same fault-tolerance guarantees as 
traditional \paxos protocols. \xxx has the potential to serve as an effective 
research template for other replication areas (\eg, byzantine fault-tolerance). 
In addition, a fast, general SMR service has been long persued as a fundamental 
building block for the emerging datacenter operation system.

% P10: Engineering contribution. Potential to substitue customized replication 
% in commodity systems and use a general ones. Easy to verify, easy to get 
% right, easy to use.
Our major engineering contribution includes the \xxx implemention and its 
evaluation on \nprog diverse, widely used server programs. Due to the lack of a 
general SMR system, industrial developers have spent tremondous efforts on 
building specific replication systems for their own programs and ``invent the 
wheels again and again". Note that understanding, building, and maintaining a 
usable SMR systems requires extreme expert knowledege, burdens, and are 
extremely challenging (so many \paxos papers). For example, the \redis or 
\memcached lag bug. Our \xxx system and evaluation has shown promising results 
on building a fast, general, and extendible SMR system and help developers 
greatly release these burdens. We have released all \xxx's source code, 
benchmarks, and raw evaluation results at \github.

% P11: Remaining of paper.
The remaining of this paper is organized as follows. \S\ref{sec:background} 
introduces background on \paxos and RDMA features. \S\ref{sec:overview} gives 
an overview of our \xxx system with key components. \S\ref{sec:input} presents 
our input coordination protocol. \S\ref{sec:output} output checking 
protocol. \S\ref{sec:impl} introduces implementation details.  
\S\ref{sec:discuss} \xxx's discusses limitations and applications. 
\S\ref{sec:evaluation} presents evaluation results, \S\ref{sec:related} 
discusses related work, and \S\ref{sec:conclusion} concludes.   