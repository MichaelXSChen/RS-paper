\section{Introduction} \label{sec:intro}

%P1: SMR difinition; traditional network message passing; reliable; attractive 
% for general servers. Agree-execute: must reach consensus and then execute a 
% request. Emphasis ordering services, Scatter, 8~12 nodes.
Consensus protocols (typically, 
\paxos~\cite{paxos:practical,paxos,paxos:simple,paxos:complex}) plays a core 
role in datacenters and distributed systems, including ordering 
services~\cite{ellis:thesis,manos:hotdep10,scatter:sosp11}, leader 
election~\cite{zookeeper}, lock services~\cite{chubby:osdi}, and 
fault-tolerance~\cite{eve:osdi12,rex:eurosys14,crane:sosp15}. A \paxos service 
runs the same program on a group of replicas and can enforce a strongly 
consistent, total order of inputs for this program as long as a quorum 
(typically, majority) of replicas still work normally.

% TBD: Scatter description is not very clear; on replica sub key range part.
Due to \paxos's strong consistency and fault-tolerance, \paxos is widely served 
in numerous systems. For instance, a DHT system Scatter~\cite{scatter:sosp11} 
partitions distinct key ranges to \paxos groups and runs 8 to 12 replicas 
in each group to agree on input requests for each key range. To improve 
throughput, Scatter further partitions the per-group key range for the replicas 
in each group so that each replica can server requests in parallel. The more 
replicas are in each group, the higher throughput Scatter can achieve. Moreover, 
recent state machine replication (SMR) systems~\cite{ crane:sosp15, eve:osdi12, 
rex:eurosys14} use \paxos to greatly improve the availability of general server 
programs.




% Typically, \paxos assigns a replica as the leader to propose 
% consensus requests, and the other replicas agree or reject requests. 
% An input consensus can achieve as long as a majority of replicas 
% agree, thus SMR can tolerate various faults such as minor replica failures.


% P2: Performance too slow. Agree first and then execute. Even three nodes, one 
% round-trip (~400 us). Not for performance critical servers such as key-value.
% Batching: addressed throughput but not latency.
Unfortunately, despite these great advances, the high consensus latency of 
traditional \paxos protools still makes many systems suffer. To achieve a total 
order of inputs efficiently, typicall \paxos deployments use a leader replica 
to determine this order. To agree on an input, at least one message round-trip 
is required between the leader and a non-leader. Given that a \v{ping} in 
Ethernet takes hundreds of \us, a server program running in an SMR 
system with three replicas must wait at least this time before processing an 
input. This latency could be acceptable for infrequent leader 
elections~\cite{chubby:osdi,zookeeper} or heavyweight 
transactions~\cite{crane:sosp15,eve:osdi12}, but prohibitive for key-value 
stores. To address this challenge, 
some systems~\cite{calvin:sigmod12,rex:eurosys14} batch requests into one 
consensus round. However, batching will only mitigate throughput lost and it 
will aggravate request latency. 
% As a possible consequence, although many recent storage 
% systems~\cite{drtm} 
% explicitly stated that they needed a replication system for high availability, 
% they finely didn't adopt the batching approach.

% P3: Another problem: scalability. As more nodes are in replica group, it is 
% getting much more slower to reach quorum. Event-driven to increase 
% parallilism, but still slow: despite the large latency, context switches (400 
% us).

As the replica group size grows, the consensus latency of traditional \paxos 
protocols increases drastically because now a majority involves more replicas. 
To improve the scalability of consensus latency, one approach is invoking 
consensus in parallel. For instance, S\-Paxos and Zookeeper use 
multithreading~\cite{spaxos, zookeeper}, and Crane~\cite{crane:sosp15} and 
libPaxos~\cite{libpaxos} use asynchronous IO (Libevent~\cite{libevent}). 
However, the high latency of a each round-trip still exists, and the 
synchronizations in these mechnisms will frequently invoke OS events such as 
context switches (each may take sub milli seconds). We ran these four 
\paxos-like protocols on 40Gbps network with only one client sending requests, 
and we found that: when increasing the replica group size from three to nine, 
their consensus latency increased by \tradlatencyincreaselow to 
\tradlatencyincreasehigh, and \systemcostlow to \systemcosthigh of this 
increase was spent in OS kernels and networking libraries.

Another approach to scale \paxos is maintaining multiple instances of \paxos 
and exploit parallilism among instances. Such approach includes partitioning a 
program and its data~\cite{scatter:sosp11,dssmr,ssmr}, splitting consensus 
loads~\cite{mencius:osdi08,spaxos}, and 
hieratical replication~\cite{manos:hotdep10,scatter:sosp11}. However, the core 
building block in these systems, \paxos itself, still scales 
poorly~\cite{ellis:thesis,scatter:sosp11}.

%  advanced 
% replication 
% models are proposed, including 
% multi-leader~\cite{epaxos:sosp13,mencius:osdi08}, 
% cluster~\cite{manos:hotdep10}, and nested consensus 
% models~\cite{scatter:sosp11}. 

% One 
% scalling approach (\eg, ~\cite{crane:sosp15}) may be using an event-driven 
% model (\eg, Libevent~\cite{libevent}) to improve the parallilism of replicas' 
% consensus round-trips. However, the high latency of a single round-trip still 
% exists.
% and synchronization context switches (often takes hundreds of \us) in 
% the event loop of this model also adds latency.


% The second challenge is that an automated, fine-grained approach is needed to 
% avoid execution divergence of active (\ie, alive) replicas. Even in the absence 
% of replica failures or network partitions, the executions of different replicas 
% can still diverge due to contention of 
% inter-thread resources~\cite{coredet:asplos10} (\eg, shared memory) and systems 
% resources~\cite{racepro:sosp11} (\eg, files and network ports). This challenge 
% not only lies in standard SMR systems which require deterministic executions, 
% but it is also pervasive in commodity replication systems (\eg, \redis, 
% \memcached, and \mysql) that seek for fault-tolerance in some degree.

% P4.0: opportunity, RDMA. We argue that, network layers are not inherent.
Fortunately, as Remote Direct Access Memory (RDMA) becomes increasing 
commonplace, it becomes a possible solution to tackle the \paxos's consensus 
latency, because it not only provides the option to bypass the OS kernel, but 
also provides dedicated, efficient TCP/IP hardware mechanisms. For instance, 
the fastest RDMA operation allows a process to directly write to the user space 
memory of a remote replica's process, completely bypassing the remote OS kernel 
or CPU (the so called ``one-sided" operations). As a common RDMA practice, to 
ensure that such a write successfully resides in the memory of a remote 
process, the local process should wait until the remote NIC (network interface 
card) sends an ACK to the local host's NIC. Such a write round-trip takes only 
$\sim$3 \us in an evaluation~\cite{pilaf:usenix14}.


% A strawman approach: DARE. RDMA communication primitives themselves have 
% scalability issues.
However, due to the unrichness of RDMA primitives, it's technically challenging 
to build a \paxos runtime system that fully exploits RDMA speed. For 
instance, one-sided RDMA operations eliminate remote replicas' participations, 
but traditional \paxos protocols require non-leader replicas to examine the 
leader's consensus requests. To overcome this issue, DARE~\cite{dare:hpdc15}, a 
recent RDMA-based \paxos protocol, proposes a sole-leader, two-round protocol. 
First, the leader uses RDMA to write the consensus requests to all 
replicas and polls RDMA ACKs to check whether the writes succeed. Second, for 
the successful writes, the leader does another round of RDMA writes to 
mark the writes as successful on other replicas, and poll ACKs on these writes. 
One a majority of successful writes in the second round, DARE reaches a 
consensus. Our evaluation shows that both the polling of RDMA ACKs and the 
two-round consensus incurred approximately linearly consensus latency: DARE's 
consensus latency increased by \darescalability as replica group size increased 
by 35x (\S\ref{evaluation}).

% Our key idea: pure remote-memory consensus. A fully scalable RDMA Paxos 
% should allow leader to process requests pure on memory. Leader and replica 
% % join % consensus; have stable storage with this benefit.
% Why is this idea fast. Mulithreading.

Our key idea is that we should make all \paxos replicas receive consensus 
messages purely on their local memory. By doing so, both the leader and 
non-leaders can receive consensus messages purely on their local memory, 
bypassing various inbound scalability bottlenecks, including RDMA ACKs and RDMA 
queue accesses. An analogy is that threads receive other threads' data and 
signals via bare memory, a fast and scalable multithreading pattern. Now the 
only RDMA primitive our \paxos replicas involve is just sending RDMA writes 
(\ie, copying the data to be sent to NIC). Our evaluation showed that 
most of such outbound RDMA write operations took less than 0.2 \us, much faster 
faster than inbound RDMA operations.

% Why is this idea feasible. Paxos can already handle unreliability.
% Tech challenge? data integrity. Storage. Checkpoints? Others?
In deed, this idea appears to pose reliability issues because now the leader 
lacks evidence on whether the remote RDMA writes succeed. Fortunately, the 
\paxos protocol already tolerates various reliability issues, including message 
losses caused by hardware or software failures. A scalable RDMA-based \paxos 
runtime system now just needs to carefully ensure the atomicity and integrity 
of RDMA writes among replicas (\S\ref{sec:normal}).



% As a common RDMA practice, to ensure that such a write 
% successfully resides in the memory of a remote process, the local process 
% should wait until the remote NIC (network interface card) sends an ACK to the 
% local host's NIC. An evaluation~\cite{pilaf:usenix14} shows that such a write 
% round-trip takes only $\sim$3 \us in the Infiniband networking
% architecture~\cite{infiniband}.

% However, it is technically challenging to fully exploit RDMA speed in \paxos 
% protocols due to the unrichness of RDMA features. We present this challenge in 
% detail by elaborating two possible approaches below. One straightforward 
% approach is IP over Infiniband (IPoIB). This approach emulates TCP/IP on RDMA 
% hardware so that traditional \paxos implementations can enjoy RDMA speedup 
% without modifications. However, this loose combination of RDMA and \paxos is 
% still one order of magnitude slower than fastest RDMA operations because IPoIB 
% goes through the OS kernel and copies network data between kernel and user 
% space.

% To the best of our knowledege, DARE's approach achieves the fastest consensus 
% speed in existing approaches because all communications are simply replaced 
% with the fatest RDMA writes (although we argue that a stable storage for 
% consensus requests should be added to ensure \paxos durability). 

% However, this 
% approach faces a scalability challenge: to ensure a remote replica is alive, 
% each step has to wait ACKs from the previous step before it starts, and each 
% RDMA write has to wait for its own ACK. In this pure leader-based algorithm, 
% ACKs are necessary for every next step to start. As the replica group size
% grows, the leader has to do RDMA writes to remote replicas one by one, making 
% its consensus latency grows linearly to replica group size (confirmed in our 
% evaluation).
% 
% to address this scalability challenge is that 
% simply replacing RDMA writes with \paxos communications is not sufficient, and
% In addition to mitigating consensus latency, RDMA creates 
% new opportunity to address the \paxos scalability problem, because we
% can integrate RDMA features \emph{tightly} within the fault-tolerant nature of 
% \paxos. In essence, \paxos already tolerates various faults, including 
% machine failures and process crashes. Therefore, we can safely ignore the ACKs 
% in RDMA writes and let \paxos handle the (un)reliability of these writes.
% 
% This integration of \paxos and RDMA features looks simple, but it leads to 
% a fast, scalable \paxos consensus algorithm with three steps. First, the leader 
% stores a consensus request in local stable storage. Second, it does RDMA writes 
% in parallel to put this request to the memory of remote replicas without 
% waiting any RDMA ACKs. Remote replicas also work in parallel: they poll from 
% their local memory, store this request in local storage, and send consensus 
% replies to the leader with RDMA writes, without waiting any RDMA ACKs 
% either. Third, once the leader sees a majority of replies in local memory, 
% a consensus is reached.
% 
% In the second step of this algorithm, both the leader and remote replicas work 
% in parallel, thus a complete consensus latency approximately consists of 
% three operations: a leader's write to stable storage, a remote replica's write 
% to local storage, and a RDMA write round-trip. This consensus 
% latency is no longer firmly correlated with replica group size (confirmed in 
% our evaluation); its scalability is now mainly bounded by the capacity of 
% outbound RDMA writes in the NIC hardware. By making the core of \paxos 
% scalable, other advanced replication 
% models~\cite{epaxos:sosp13,mencius:osdi08,scatter:sosp11,manos:hotdep10} can 
% scale even better.
%  (currently, 16~\cite{herd:sigcomm})

% P4: Falcon; key features. Hook sockets in servers.
We have adopted this idea in \xxx,\footnote{We name our system after 
falcon, one of the astest birds.} a fast, scalable \paxos protocol and its 
runtime system. \xxx supports general programs: within \xxx, a program just 
runs as is, and \xxx automatically deploys this program on replicas of machines. 
It intercepts inputs from a server program's inbound socket calls (\eg, \recv) 
and invokes our \paxos protocol to efficiently enforce same inputs across 
replicas.

To practically improve the assurance that replicas run in sync, on top of 
\xxx's \paxos protocol, we also build an efficient network output checking 
protocol that efficiently compares output across replicas. first computing an 
accumulated hash by intercepting a server program's outbound socket calls (\eg, 
\send), it then occasionally invokes an consensus to compare these hashes among 
replicas. This output checking protocol is just a practical feature that could 
improve assurance on keeping replicas in sync and promote \xxx's deployments. 

% \xxx then provides an optional 
% rollback/restore mechanism to make an effort to restore the diverged replicas. 
% Because hash computation is efficient and output consensus is invoked rarely, 
% this output checking protocol introduced negligible performance impact in our 
% evaluation.

% In a consensus protocol level, \xxx carefully tackles several technical 
% challenges, including handling an atomicity challenge (\S\ref{sec:normal}) and 
% concurrent connections (\S\ref{sec:concurrent}).

% If \xxx finds that 
% a replica produces an different output from what other replicas agree on, \xxx 
% recovers this replica to a previous program checkpoint and re-executes inputs 
% that have been agreed on from the checkpoint. 

% P6: Falcon: output checker.
% However, to practically replicate general server programs, only enforcing same 
% inputs is often not enough. An automated, efficient output checking mechanism 
% that can improve the assurance on ``replicas run in sync" is still missing in 
% existing SMR 
% systems~\cite{calvin:sigmod12,rex:eurosys14,crane:sosp15,dare:hpdc15}. 
% Most server programs use multithreading to harness the power of multi-core 
% hardware. Nondeterminism~\cite{racepro:sosp11,dmp:asplos09,coredet:asplos10,
% cui:tern:osdi10, kendo:asplos09,
% dthreads:sosp11,peregrine:sosp11,parrot:sosp13,determinator:osdi10} caused by 
% contentions in inter-thread resources (\eg, global memory and locks) and systems 
% resources (\eg, network ports) can easily cause program execution states to 
% diverge across replicas and compute wrong outputs to clients.
% 
% To tackle nondeterminism, SMR systems either use deterministic multithreading 
% and replay approaches~\cite{rex:eurosys14,crane:sosp15,ddos:asplos13}, or they 
% rely on manually annotating share states in program code to detect execution 
% divergence~\cite{eve:osdi12}. These approaches fall short in performance 
% or automation.



% This 
% protocol automatically, efficiently checks the fine-grained network outputs 
% and improves assurance on whether replicas run in sync.

% A practical 
% output checking mechanism is missing in widely deployed replication 
% systems (\eg, \redis and \mysql) either, although these replication sytems 
% provide weaker fault-tolerance or consistency guarantees than SMR for better 
% performance. 
% 
% Two approaches for checking whether replicas run in sync exists. Existing 
% widely deployed systems typically use \v{ping} to check whether replicas run in 
% sync, but this coarse-grained checking will miss output divergence caused by 
% tricky concurrency bugs~\cite{lu:concurrency-bugs}. Eve 

% First, introduce naive approach. IPoverIB.

% P5: Falcon: RDMA input coordination. Persistent stores; two RDMA writes 
% between two machines; no context switch.
% To coordinate inputs among replicas, \xxx intercepts a server program's socket 
% APIs (\eg, \recv) to caputure inputs and introduces a new RDMA-accelerated 
% \paxos protocol to let replicas agree on these inputs. To ease understanding 
% and checking tooks. this protocol complies with common style of popular paxos 
% protocal~\cite{paxos:practical}. In the normal case of this protocol, contrast 
% to existing implementations which require one network round-trip (\ie, two 
% messages for every two replicas), our protocal only requires two most efficient 
% one-sided write operations.

% P5.1: Support read-only optimization.



% To address this challenge, recent SMR systems leverage either deterministic 
% multithreading techniques~\cite{rex:eurosys14,crane:sosp15} or detecting 
% divergence of execution by manually annotating program states by threads, 
% artificially trading off performance or automatacity.

% Typical commodity 
% replication systems ignore this challenge and use `ping" to check whether 
% replicas are working as expected, but this coarse-grained approach can not 
% detect execution divergence of resource contentions because a program can just 
% compute wrong outputs without crashing. 

% Our key idea is that we don't need to a program's every (or every batch) 
% network outputs because they most replicas's outputs indicate that this output 
% is most likely the produced one. Either necessary or sufficient. Not necessary 
% because most executions already produce same program behaviors (including 
% outputs) even with concurrency bugs. Not sufficient because it could be all 
% replicas producing the same buggy output and bypass consensus protocols. All we 
% need is just lazily compare outputs and if a divergence is detected, we roll 
% back programs and re-execute them.

% To implement this idea, \xxx's output verification protocol first . network 
% outputs on each individual replica, computes  hash values incremental:
% compute the hash value of a union of last hash value and the output  and 
% periodically invoke our \paxos consensus protocol to exchange the hash value. 
% Then, if minor replicas' outputs diverge from the majority ones, we just roll 
% back and re-execute these minor replicas without perturbing the others to agree 
% on and process new inputs. If a majority can not reach, \xxx simply rolls back 
% the XXX (leader?). Evaluation confirmed that XX.XX\% cases.


% P7: conceptual level: complete architecture. agree-execute-enforcement.
% In a conceptual level, to provide pratical SMR service for general programs, 
% \xxx presents a new agree-execute-verify execution model, which contrasts from 
% previous agree-execute models and execute-verify models. We argue that agree is 
% essential to SMR due to its strong fault-tolerance on machine failures and 
% packet losses (even RDMA networks have packet loss when machines fail or 
% programs crash). Having a general input coordination protocol also mitigates 
% the need of writing application-specific input mixer and manually code 
% annotation. Moreover, a automatic, fast output verification protocol is 
% essential to SMR because we aim to replicate general, diverse server programs 
% that may diverge due to resource contentions. In sum, by coordinating inputs 
% and verifying outputs among replicas, \xxx practically enforces same execution 
% states and outputs among replicas.

% P8: implementation. POSIX. support checkpoint.
We implemented \xxx in Linux. \xxx intercepts POSIX inbound socket calls 
(\eg, \accept and \recv) to coordinate inputs using the Infiniband 
RDMA architecture. \xxx intercepts POSIX outbound socket operations (\eg, 
\send) to invoke the output checking protocol. This simple, deployable 
interface design makes \xxx support general server programs without modifying 
them. To recover or add new replicas, \xxx leverages \criu~\cite{criu} to 
perform checkpoint/restore for general server programs on one non-leader 
replica, introducing little performance impact in normal case.

% P9: Evaluatuion, with highlight items, match abstract, but more details.
We compared \xxx with five popular, open source \paxos-like implementations, 
including four traditional ones (\libpaxos~\cite{libpaxos}, 
\zookeeper~\cite{zookeeper}, \crane~\cite{crane:sosp15} and 
\spaxos~\cite{spaxos}) and a RDMA-based one (\dare~\cite{dare:hpdc15}). \spaxos 
is designed to achieve scalable throughput when more replicas are added. We 
also evaluated \xxx on \nprog widely used or studied server programs, including 
\nkvprog key-value stores (\redis~\cite{redis}, \memcached~\cite{memcached}, 
\ssdb~\cite{ssdb}, and \mongodb~\cite{mongodb}), one SQL server 
\mysql~\cite{mysql}, one anti-virus server \clamav~\cite{clamav}, one multimedia 
storage server \mediatomb~\cite{mediatomb}, one LDAP server 
\openldap~\cite{openldap}, and one advanced transactional database 
\calvin~\cite{calvin:sigmod12} (with \zookeeper~\cite{zookeeper} as its SMR 
protocol). Our evaluation shows that

\begin{tightenum}
\item \xxx achieves both one order of magnitude better scalability and one 
order of magnitude faster consensus latency than literature. 
Figure~\ref{fig:summary} shows a summary. \xxx's consensus latency was faster 
than four popular \paxos implimentations by \comptradlow to \comptradhigh on 
three to nine replicas. \xxx is faster than \dare by \fasterDARElow to 
\fasterDARE. When increasing the replica group size from three to 105 (a 35x 
increase), \xxx's consensus latency increases merely from \xxxlatencythree to 
\xxxlatencyonezerofive (a \xxxscalability, sub-linear increase).

\item \xxx is general. For all \nprog evaluated programs, \xxx ran them without 
any modification except \calvin (we added a \nlinescalvin-line patch to make 
\calvin's client and server communicate with sockets).

\item \xxx incurs low overhead on \nprog widely used server programs. 
With nine replicas, compared to servers' own unreplicated executions, \xxx 
incurred merely \tputoverhead overhead on throughput and \latencyoverhead on 
response time in average.

\item \xxx is robust. On \paxos leader failures, \xxx's leader election 
latency was reasonable and scalable.

% \xxx's consensus latency is \fasterthanzookeeper 
% faster than \calvin's SMR service \zookeeper.





% \item \xxx is extensible. To extend optimization on read-only requests, XX 
% lines of code in our two provided APIs, \xxx is able to avoid the read-only 
% requests in \redis to do consensus and XX times faster than \redis's own 
% replication system. 

\end{tightenum}  
% % tighten items, highlighted.

% P10: Conceptual contribution. Applications: other replications, parallel 
% program % analysis, and datacenter OS (it's efficiency and strong consistency 
% makes % system calls go beyond single machine).
% New design space. comprehensive model. many applications: other replications, 
% parallel analysis, datacenter OS.
Our major contribution is the idea of pure remote-memory consensus. This simple 
yet effective idea leads to \xxx, a fast, scalable \paxos runtime system. \xxx 
has the potential to largely improve the scale and speed of existing \paxos 
services. For instance, previously Scatter deployed 8 to 12 replicas in each 
\paxos group~\cite{scatter:sosp11}, now it can deploy one order of magnitude 
more replicas in each group with much faster consensus latency. Moreover, a 
general and deployable service, \xxx may largely promote the deployments of 
\paxos and provide strong fault-tolerance and consistency to various 
systems.

% \xxx 
% can also be applied to broad areas, including other replication protocols (\eg, 
% byzantine fault-tolerance~\cite{zyzzyva:sosp07,pbft:osdi99}), distributed 
% program analyses, and future datacenter operating systems (\S\ref{sec:apps}). 
% All \xxx source code, benchmarks, and evaluation results are 
% available at \github. 
% In addition, a fast, general SMR service has been long persued as a fundamental 
% building block for the emerging datacenter operation system.

% P10: Engineering contribution. Potential to substitue customized replication 
% in commodity systems and use a general ones. Easy to verify, easy to get 
% right, easy to use.
% Our major engineering contribution includes the \xxx implemention and its 
% evaluation on \nprog diverse, widely used server programs. Due to the lack of a 
% general SMR system, industrial developers have spent tremondous efforts on 
% building specific replication systems for their own programs and ``invent the 
% wheels again and again". Note that understanding, building, and maintaining a 
% usable SMR systems requires extreme expert knowledege, burdens, and are 
% extremely challenging (so many \paxos papers). For example, the \redis or 
% \memcached lag bug. Our \xxx system and evaluation has shown promising results 
% on building a fast, general, and extendible SMR system and help developers 
% greatly release these burdens. We have released all \xxx's source code, 
% benchmarks, and raw evaluation results at \github.

% P11: Remaining of paper.
The remaining of this paper is organized as follows. 
\S\ref{sec:background} introduces background on \paxos and RDMA features. 
\S\ref{sec:overview} gives an overview of our \xxx system. \S\ref{sec:input} 
presents \xxx's consensus protocol and its runtime system. \S\ref{sec:output} 
describes the output checking protocol. \S\ref{sec:discuss} compares \dare with 
\xxx, and discusses \xxx's current limitations and applications in other areas. 
\S\ref{sec:evaluation} presents evaluation results, \S\ref{sec:related} 
discusses related work, and \S\ref{sec:conclusion} concludes.   