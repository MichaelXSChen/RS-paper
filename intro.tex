\section{Introduction} \label{sec:intro}

%P1: SMR difinition; traditional network message passing; reliable; attractive 
% for general servers. Agree-execute: must reach consensus and then execute a 
% request.
State machine replicaion (SMR) runs the same program on a 
number of replicas and uses a distributed consensus protocol (\eg, 
\paxos~\cite{crane:sosp15}) to enforce the same inputs among 
replicas. Typically, \paxos assigns a replica as the leader to propose 
consensus requests, while the other replicas agree or reject requests. 
Consensus on a new input can be achieved as long as a majority of replicas 
agree, thus SMR can tolerate various faults such as minor replica 
failures. Attracted by this strong fault-tolerance, recently, several SMR 
systems~\cite{chubby:osdi, zookeeper, crane:sosp15, eve:osdi12, rex:eurosys14} 
have been built to greatly improve the availability of various server programs, 
because these programs tend to serve client requests at all time.

% P2: Performance too slow. Agree first and then execute. Even three nodes, one 
% round-trip (~400 us). Not for performance critical servers such as key-value.
% Batching: addressed throughput but not latency.
Unfortunately, despite these recent advances, SMR remains difficult to be 
widely adopted due to the high consensus latency in \paxos. To agree on an 
input, traditional consensus protocols invoke at least one message round-trip 
between two replicas. Given that a \v{ping} in Ethernet takes hundreds of \us, a 
server program running in an SMR system with only three replicas must wait at 
least this time before processing an input. This latency may be acceptable for 
maintaining global configuration~\cite{chubby:osdi,zookeeper} or processing SQL 
transactions~\cite{crane:sosp15,eve:osdi12}, but prohibitive for 
key-value stores. To mitigate this challenge, some recent SMR 
systems~\cite{calvin:sigmod12,rex:eusorys14} batch requests into one 
consensus round. However, batching can only mitigate a server's throught lost; 
it may aggravate response time. As the replica group size grows, consensus 
protocols are even more difficult to scale, because a majority involves more 
replicas with more round-trips.
% As a possible consequence, although many recent storage 
% systems~\cite{drtm} 
% explicitly stated that they needed a replication system for high availability, 
% they finely didn't adopt the batching approach.

% P3: Another problem: scalability. As more nodes are in replica group, it is 
% getting much more slower to reach quorum. Event-driven to increase 
% parallilism, but still slow: despite the large latency, context switches (400 
% us).


% One 
% scalling approach (\eg, ~\cite{crane:sosp15}) may be using an event-driven 
% model (\eg, Libevent~\cite{libevent}) to improve the parallilism of replicas' 
% consensus round-trips. However, the high latency of a single round-trip still 
% exists.
% and synchronization context switches (often takes hundreds of \us) in 
% the event loop of this model also adds latency.


% The second challenge is that an automated, fine-grained approach is needed to 
% avoid execution divergence of active (\ie, alive) replicas. Even in the absence 
% of replica failures or network partitions, the executions of different replicas 
% can still diverge due to contention of 
% inter-thread resources~\cite{coredet:asplos10} (\eg, shared memory) and systems 
% resources~\cite{racepro:sosp11} (\eg, files and network ports). This challenge 
% not only lies in standard SMR systems which require deterministic executions, 
% but it is also pervasive in commodity replication systems (\eg, \redis, 
% \memcached, and \mysql) that seek for fault-tolerance in some degree.

% P4.0: opportunity, RDMA. We argue that, network layers are not inherent.
Remote Direct Access Memory (RDMA) is a promising technique to mitigate 
consensus latency because recently it becomes cheaper and increasingly 
pervasive in datacenters. RDMA allows a host machine to directly write to the 
memory of a remote host, completely bypassing the OS kernel or CPU at 
the remote host (the so called one-sided RDMA write operations). As a common 
RDMA practice, to ensure the remote host is alive and such a write succees, the 
local host should wait until the remote NIC to send an ACK to the local host's 
NIC. An evaluation~\cite{pilaf:atc14} shows that such a write round-trip takes 
only $\sim$3 \us in the Infiniband architecture~\cite{infiniband}.

% However, it is technically challenging to fully exploit RDMA speed in \paxos 
% protocols due to the unrichness of RDMA features. We present this challenge in 
% detail by elaborating two possible approaches below. One straightforward 
% approach is IP over Infiniband (IPoIB). This approach emulates TCP/IP on RDMA 
% hardware so that traditional \paxos implementations can enjoy RDMA speedup 
% without modifications. However, this loose combination of RDMA and \paxos is 
% still one order of magnitude slower than fastest RDMA operations because IPoIB 
% goes through the OS kernel and copies network data between kernel and user 
% space.

% To the best of our knowledege, DARE's approach achieves the fastest consensus 
% speed in existing approaches because all communications are simply replaced 
% with the fatest RDMA writes (although we argue that a stable storage for 
% consensus requests should be added to ensure \paxos durability). 

% However, this 
% approach faces a scalability challenge: to ensure a remote replica is alive, 
% each step has to wait ACKs from the previous step before it starts, and each 
% RDMA write has to wait for its own ACK. In this pure leader-based algorithm, 
% ACKs are necessary for every next step to start. As the replica group size
% grows, the leader has to do RDMA writes to remote replicas one by one, making 
% its consensus latency grows linearly to replica group size (confirmed in our 
% evaluation).
% 
% to address this scalability challenge is that 
% simply replacing RDMA writes with \paxos communications is not sufficient, and
In addition to mitigating the one-hop consensus latency, RDMA also creates 
new opportunity to address the scalability problem in \paxos, because our key 
observation is that we can integrate RDMA writes even \emph{more tightly} with 
the fault-tolerant nature of \paxos. In essense, \paxos has already tolerated 
various faults, including machine failures and packet losses (even reliable 
connections in RDMA may lose packets during hardware and program failovers). 
Therefore, we can safely ignore the ACKs in RDMA writes and rely on \paxos to 
handle the reliability issues of these writes.

This tight integration of \paxos and RDMA writes looks simple, but it leads to 
a new fast, scalable \paxos consensus algorithm with three steps in normal 
case. First, the leader stores a consensus request in local stable storage 
(SSD). Second, it does parallel RDMA writes to write this request to the remote 
memory of the other replicas without waiting any RDMA ACKs. Remote replicas 
also work in parallel: they poll from their local memory, store this request in 
local storage, and send consensus replies to the leader rapidly with RDMA 
writes, without waiting any RDMA ACKs either. Third, once the leader sees a 
majority of replies in its local memory, a consensus is reached.

In the second step of this algorithm, both the leader and remote replicas work 
in parallel, so a whole consensus latency aproximately consists of 
three operations: a leader's write to stable storage, a remote replica's write 
to local storage, and a single RDMA write round-trip. This consensus latency is 
no longer linear to replica group size (confirmed in our evaluation); its 
scalability is now mainly bounded by the max number of outbound RDMA writes in 
the RDMA NIC hardware.
%  (currently, 16~\cite{herd:sigcomm})


% P4: Falcon; key features. Hook sockets in servers.
This paper present \xxx, an SMR system that replicates general server programs 
efficiently. With \xxx, a server program just runs as is, and \xxx 
automatically deploys this program on replicas of machines. \xxx intercepts 
inputs from the inbound socket calls (\eg, \recv) of a server program and 
invokes our new consensus algorithm to enforce same inputs across replicas.

% If \xxx finds that 
% a replica produces an different output from what other replicas agree on, \xxx 
% recovers this replica to a previous program checkpoint and re-executes inputs 
% that have been agreed on from the checkpoint. 

% P6: Falcon: output checker.
However, to practically replicate general server programs, only enforcing same 
inputs is often not enough. An automated, efficient output checking mechanism 
that can improve the assurance on ``replicas run in sync" is still missing in 
existing SMR 
systems~\cite{calvin:sigmod12,rex:eurosys14,crane:sosp15,dare:hpdc15}. Server 
programs now already use multithreading to harness the power of multi-core 
hardware. Nondeterminism~\cite{racepro:sosp11,dmp:asplos09,coredet:asplos10,
cui:tern:osdi10, kendo:asplos09,
dthreads:sosp11,peregrine:sosp11,parrot:sosp13,determinator:osdi10} caused by 
contentions in inter-thread resources (\eg, global memory and locks) and systems 
resources (\eg, network ports) can easily cause program execution states to 
diverge across replicas and compute wrong outputs to clients. Existing 
replication approaches either use only \v{ping} to check the liveness of 
replicas (\eg, \redis~\cite{redis}) or rely on manually annotating share states 
in program code to detect execution divergence~\cite{eve:osdi12}.

On top of the input consensus protocol, \xxx builds an efficient output 
checking protocol that occasionally checks output divergence across replicas. 
This checking protocol works by first computing a accumulated hash for outbound 
socke calls (\eg, \send) in a server program, it then occasionally invokes an 
output consensus by collecting the hashes in replicas' consensus replies for 
the leader. \xxx then provides an optional rollback and restore mechanism for 
program deployers to make an effort to restore the diverged replicas.

% This 
% protocol automatically, efficiently checks the fine-grained network outputs 
% and improves assurance on whether replicas run in sync.

% A practical 
% output checking mechanism is missing in widely deployed replication 
% systems (\eg, \redis and \mysql) either, although these replication sytems 
% provide weaker fault-tolerance or consistency guarantees than SMR for better 
% performance. 
% 
% Two approaches for checking whether replicas run in sync exists. Existing 
% widely deployed systems typically use \v{ping} to check whether replicas run in 
% sync, but this coarse-grained checking will miss output divergence caused by 
% tricky concurrency bugs~\cite{lu:concurrency-bugs}. Eve 

% First, introduce naive approach. IPoverIB.

% P5: Falcon: RDMA input coordination. Persistent stores; two RDMA writes 
% between two machines; no context switch.
% To coordinate inputs among replicas, \xxx intercepts a server program's socket 
% APIs (\eg, \recv) to caputure inputs and introduces a new RDMA-accelerated 
% \paxos protocol to let replicas agree on these inputs. To ease understanding 
% and checking tooks. this protocol complies with common style of popular paxos 
% protocal~\cite{paxos:practical}. In the normal case of this protocol, contrast 
% to existing implementations which require one network round-trip (\ie, two 
% messages for every two replicas), our protocal only requires two most efficient 
% one-sided write operations.

% P5.1: Support read-only optimization.



% To address this challenge, recent SMR systems leverage either deterministic 
% multithreading techniques~\cite{rex:eurosys14,crane:sosp15} or detecting 
% divergence of execution by manually annotating program states by threads, 
% artificially trading off performance or automatacity.

% Typical commodity 
% replication systems ignore this challenge and use `ping" to check whether 
% replicas are working as expected, but this coarse-grained approach can not 
% detect execution divergence of resource contentions because a program can just 
% compute wrong outputs without crashing. 

% Our key idea is that we don't need to a program's every (or every batch) 
% network outputs because they most replicas's outputs indicate that this output 
% is most likely the produced one. Either necessary or sufficient. Not necessary 
% because most executions already produce same program behaviors (including 
% outputs) even with concurrency bugs. Not sufficient because it could be all 
% replicas producing the same buggy output and bypass consensus protocols. All we 
% need is just lazily compare outputs and if a divergence is detected, we roll 
% back programs and re-execute them.

% To implement this idea, \xxx's output verification protocol first . network 
% outputs on each individual replica, computes  hash values incremental:
% compute the hash value of a union of last hash value and the output  and 
% periodically invoke our \paxos consensus protocol to exchange the hash value. 
% Then, if minor replicas' outputs diverge from the majority ones, we just roll 
% back and re-execute these minor replicas without perturbing the others to agree 
% on and process new inputs. If a majority can not reach, \xxx simply rolls back 
% the XXX (leader?). Evaluation confirmed that XX.XX\% cases.


% P7: conceptual level: complete architecture. agree-execute-enforcement.
% In a conceptual level, to provide pratical SMR service for general programs, 
% \xxx presents a new agree-execute-verify execution model, which contrasts from 
% previous agree-execute models and execute-verify models. We argue that agree is 
% essential to SMR due to its strong fault-tolerance on machine failures and 
% packet losses (even RDMA networks have packet loss when machines fail or 
% programs crash). Having a general input coordination protocol also mitigates 
% the need of writing application-specific input mixer and manually code 
% annotation. Moreover, a automatic, fast output verification protocol is 
% essential to SMR because we aim to replicate general, diverse server programs 
% that may diverge due to resource contentions. In sum, by coordinating inputs 
% and verifying outputs among replicas, \xxx practically enforces same execution 
% states and outputs among replicas.

% P8: implementation. POSIX. support checkpoint.
We implemented \xxx in Linux. \xxx intercepts POSIX inbound socket calls 
(\eg, \accept and \recv) to coordinate inputs using the Infiniband 
RDMA architecture. \xxx intercepts POSIX outbound socket operations (\eg, 
\send) to invoke the output checking protocol. This simple, deployable 
interface design makes \xxx support general server programs without modifying 
them. To recover or add new replicas, \xxx leverages \criu~\cite{criu} to 
perform checkpoint and restore on general server programs.

% P9: Evaluatuion, with highlight items, match abstract, but more details.
We evaluated \xxx on \nprog widely used or studied server programs, including 
\nkvprog key value stores (\redis, \memcached, \ssdb, and \mongodb), one SQL 
server \mysql, one anti-virus server \clamav, one multimedia storage server 
\mediatomb, one LDAP server \openldap, and one advanced transactional database 
\calvin (with \zookeeper as its SMR protocol). Our evaluation shows that

\begin{tightenum}
\item \xxx is general. For all evaluated programs, \xxx ran them without any 
modification except \calvin (we added a \nlinescalvin patch to make its client 
and server programs communicate with sockets).

\item \xxx is fast. Compared to the \nprog servers' unreplicated executions, 
\xxx incurred merely \tputoverhead overhead on throughtput and \latencyoverhead 
on response time in average. \xxx's consensus latency is \fasterthanzookeeper 
faster than \calvin's \zookeeper-based SMR service running on IPoIB.

\item \xxx is scalable to replica group size. \xxx achieved a scalable 
consensus latency of 9.9$\sim$11.0 \us from 3 to 7 replicas, while that 
of DARE is 7.0$\sim$28.2 \us from 3 to 5 replicas.

\item \xxx is robust. Among XXX repeated executions, \xxx detected and 
recovered execution divergence caused by a software bug in \redis, while 
\redis's own replication service missed the bug.

% \item \xxx is extensible. To extend optimization on read-only requests, XX 
% lines of code in our two provided APIs, \xxx is able to avoid the read-only 
% requests in \redis to do consensus and XX times faster than \redis's own 
% replication system. 

\end{tightenum}  
% % tighten items, highlighted.

% P10: Conceptual contribution. Applications: other replications, parallel 
% program % analysis, and datacenter OS (it's efficiency and strong consistency 
% makes % system calls go beyond single machine).
% New design space. comprehensive model. many applications: other replications, 
% parallel analysis, datacenter OS.
Our major conceptual contribution is a fast, scalable \paxos consensus 
algorithm by tightly integrating RDMA write operations with the fault-tolerant 
nature of \paxos. \xxx and its implementation have the potential to largely 
increase the adoption of SMR. \xxx can also be applied to broad areas, 
including other distributed protocols, distributed program analyses, and 
future datacenter operating systems (\S\ref{sec:apps}). 
% In addition, a fast, general SMR service has been long persued as a fundamental 
% building block for the emerging datacenter operation system.

% P10: Engineering contribution. Potential to substitue customized replication 
% in commodity systems and use a general ones. Easy to verify, easy to get 
% right, easy to use.
% Our major engineering contribution includes the \xxx implemention and its 
% evaluation on \nprog diverse, widely used server programs. Due to the lack of a 
% general SMR system, industrial developers have spent tremondous efforts on 
% building specific replication systems for their own programs and ``invent the 
% wheels again and again". Note that understanding, building, and maintaining a 
% usable SMR systems requires extreme expert knowledege, burdens, and are 
% extremely challenging (so many \paxos papers). For example, the \redis or 
% \memcached lag bug. Our \xxx system and evaluation has shown promising results 
% on building a fast, general, and extendible SMR system and help developers 
% greatly release these burdens. We have released all \xxx's source code, 
% benchmarks, and raw evaluation results at \github.

% P11: Remaining of paper.
The remaining of this paper is organized as follows. \S\ref{sec:background} 
introduces background on \paxos and RDMA features. \S\ref{sec:overview} gives 
an overview of our \xxx system. \S\ref{sec:input} presents our input consensus 
protocol. \S\ref{sec:output} describes the output checking protocol. 
\S\ref{sec:discuss} \xxx's discusses current limitations and its broad 
applications in other areas. \S\ref{sec:evaluation} presents evaluation 
results, \S\ref{sec:related} discusses related work, and \S\ref{sec:conclusion} 
concludes.   