\section{Related Work} \label{sec:related}


\para{State machine replication (SMR).}  SMR is a powerful, but 
complex fault-tolerance technique. The literature has developeed a rich set of
\paxos 
algorithms~\cite{paxos:practical,paxos,paxos:simple,paxos:complex,epaxos:sosp13}
and implementations~\cite{paxos:live,paxos:practical,chubby:osdi}. \paxos is 
notoriously difficult to be fast and scalable~\cite{ellis:thesis}. To improve 
speed and scalability, various advanced replication models have been 
developed~\cite{epaxos:sosp13,mencius:osdi08,scatter:sosp11,manos:hotdep10}. 
Since consensus protocols play a core role in 
datacenters~\cite{matei:hotcloud11, mesos:nsdi11, datacenter:os} and distributed 
systems~\cite{spanner:osdi12,mencius:osdi08}, a variety of study have been 
conducted to improve different aspects of consensus protocols, including 
performance~\cite{epaxos:sosp13,paxos:fast,dare:hpdc15}, 
understandability~\cite{raft:usenix14,paxos}, and verifiable reliability 
rules~\cite{modist:nsdi09,demeter:sosp11}. Although \xxx tightly integrates 
RDMA features in \paxos, its implementation mostly complies with a popular, 
practical approach~\cite{paxos:practical} for reliability. Other \paxos 
approaches can also be leveraged in \xxx.

Five systems aim to provide SMR or similar fault-tolerance guarantees to server 
programs and thus they are the most relevant to \xxx. They can be divided into 
two categories depending on whether their protocols run on TCP/IP or RDMA. 
The first category runs on TCP/IP, including Eve~\cite{eve:osdi12}, 
Rex~\cite{rex:eurosys14}, Calvin~\cite{calvin:sigmod12}, and 
Crane~\cite{crane:sosp15}. Evaluation in these systems have shown that SMR 
services incur modest overhead on server programs' throughput compared to their 
unreplicated executions. However, for some other programs (\eg, key-value 
stores) demanding a short response time, \xxx is more suitable because these 
systems' consensus latency is one order of magnitude higher than \xxx's 
(\S\ref{sec:compare}).
% These systems can leverage \xxx's general, 
% RDMA-accelerated protocol to improve latency.

% % DARE: simply use RDMA writes to do consensus on the leader side.
% To further improve consensus speed, DARE~\cite{dare:hpdc15} proposes a second 
% approach by simply replacing message passing in \paxos with one-sided RDMA 
% operations. For speed, DARE lets the leader handle a whole consensus round
% with three steps. The leader first appends a consensus request to a consensus 
% log in all remote replicas with RDMA writes. For the the successful writes with 
% ACKs, it then updates the tail pointers in remote logs and wait ACKs of these 
% updates. Finally, the leader knows that the minimum tail pointer among at 
% least a majority of replcias reach consensus.

% Two systems Rex and Eve did not provide latency evaluation; not open source.
% Two systems Calvin and Crane, we ran them with Calvin's database; we are XX 
% times faster.

Notably, Eve~\cite{eve:osdi12} presents an execution state checking approach 
based on their \paxos coordination service. Eve's completeness on detecting 
execution divergence relies on whether developers have manually annotated all 
thread-shared states in program code. \xxx's output checking approach is 
automated (no manual code annotation is needed), and its completeness depends on 
whether the diverged execution states propagate to network outputs. Eve and 
\xxx's checking approaches are complementary and can be integrated.

% Two DARE limitations: first, no stable storage. second, old leader may 
% % unsafely writes to remote logs and thought it has reached a consensus.
The second category includes DARE~\cite{dare:hpdc15}, a coordination protocol 
that also uses RDMA to reduce latency. \xxx differs from DARE in three major 
aspects. First, in a reliability model level, DARE's model is 
different from standard \paxos's: DARE assumes that a replica's memory is still 
accessible to remote replicas even if this replica's CPU fails, so that DARE's 
leader can still write to remote backups. With this reliability model, DARE 
requires four one-sided RDMA writes in each consensus round between the leader 
and a backup. DARE's paper shows that the MTTF (mean time to failure) of memory 
and CPU is similar.

In contrast, \xxx's reliability model complies with standard \paxos's: memory 
and CPU may fail, thus consensus requests must be written to stable storage. 
Thus, \xxx requires two one-sided RDMA writes and two SSD writes on a consensus 
round between the leader and a backup. DARE reported a $\sim$15 \us consensus 
latency, and \xxx's latency is compatible with DARE's on same payload: DARE's 
payload is 64bytes, similar to \xxx's \redis consensus latency 
(\S\ref{sec:overhead}).

Second, in a protocol level, DARE does pure-leader consensus in normal case 
for efficiency. The leader writes a consensus request to remote replicas' 
logs. If a majority of writes succeed with RDMA ACKs, the leader thinks that a 
consensus has reached. DARE's backups do not invoke any proposal 
number~\cite{paxos:simple} or view~\cite{paxos:practical} checking during 
consensus.

In general, we argue that a pure-leader consensus approach is difficult to 
guarantee safety. For instance, a leader may disconnect from the network 
shortly (\eg, a temporary NIC error), reconnect, and then invoke 
consensus requests. Concurrently, backups may start a leader election. It could 
be risky that the leader's RDMA writes succeed and it ``thinks" that a 
consensus has reached (because it handles everything of a consensus without any 
backup involvement), but backups may have inconsistently started a leader 
election.

Even if a backup first carefully does a check on a heartbeat timeout (or a 
latest consensus request), it then closes the RDMA QP with this ``leader", 
these two operations still have a vulnerable time window and the ``leader" can 
write in between. Allowing a leader to ``think" that it has got a consensus 
is unsafe. This is probably why standard \paxos 
protocols~\cite{paxos,paxos:simple,paxos:practical} must let backups do proposal 
number of view checking rather than taking a pure-leader approach. \xxx complies 
with standard \paxos protocol~\cite{paxos:practical}: a \xxx backup always 
first compares its own view ID with the view ID in a consensue request, it then 
agrees or rejects this request.

Third, in a application level, DARE's evaluation used a 335-line, 
single-threaded key-value store. \xxx supports general, diverse server 
programs.

% Various paxos protocols. Ours is complementary. Can be plugged into our 
% system.

% Three systems aim to provide SMR services to server programs. Eve, Rex, Dare, 
% Crane.
% Type I: Dare. Weaker durability. Four RDMA writes. We have two RDMA writes 
% plus two SSD writes. Compatible perf. Not for general server programs.

% Type II: Eve, Rex, Crane. All slow. Eve has output check, but also slow, use 
% TCP, and needs annotation.


\para{RDMA techniques.} RDMA techniques have been leveraged in many online 
storage systems to improve latency and throughput within datacenters. These 
systems include key-value 
stores~\cite{pilaf:usenix14,herd:sigcomm14,farm:nsdi14} and 
transactional processing systems~\cite{drtm:sosp15,farm:sosp15}. These systems 
mainly aim to use RDMA to speedup specific communications, where both their 
storage and client access have RDMA enabled. \xxx's deployment model is to 
provide SMR fault-tolerance to general server programs deployed in datacenters, 
and the client programs access these server programs in LAN or WAN. It would be 
interesting to investigate whether \xxx can improve the availability for both 
the client side and server side of these advanced systems within a datacenter, 
and we leave it for future work.

% We didn't evaluate these systems because they are not available on client sides.


% Increasing assurance of replicas on thread schedulings. Not general 
% as Falcon's checkpoint techniques. But complementary for example ClamAV.
\para{Determinism techniques.} In order to make multi-threading easier to 
understand, test, analyze, and replicate, researchers have built two types of 
reliable multi-threading systems: (1) stable multi-threading systems (or 
\smt)~\cite{grace:oopsla09, dthreads:sosp11, determinator:osdi10} that aim to 
reduce the number of possible thread interleavings for program all inputs, and 
(2) deterministic multi-threading systems (or \dmt)~\cite{dpj:oopsla09, 
dmp:asplos09,kendo:asplos09,coredet:asplos10,dos:osdi10,ddos:asplos13,
ics:oopsla13} that aim to reduce the number of possible thread interleavings on 
each program input. Typically, these systems use deterministic logical clocks 
instead of nondeterministic physical clocks to make sure inter-thread 
communications (\eg, \mutexlock and accesses to global variables) can only 
happen at some specific logical clocks. Therefore, given the same or similar 
inputs, these systems can enforce the same thread interleavings and eventually 
the same executions. These systems 
have shown to greatly improve software reliability, including coverage of 
testing inputs~\cite{ics:oopsla13} and speed of recording 
executions\cite{dos:osdi10} for debugging.

% \para{Concurrency.} Pervasive in real-world applications. Threads. Processes. 
% Break SMR's state machine assumption. complementary.
\para{Concurrency.} \xxx are mutually beneficial with much prior work on 
concurrency error 
detection~\cite{yu:racetrack:sosp,savage:eraser,racerx:sosp03,lu:muvi:sosp,
avio:asplos06,conmem:asplos10},
diagnosis~\cite{racefuzzer:pldi08,ctrigger:asplos09,atomfuzzer:fse08}, and
correction~\cite{dimmunix:osdi08,gadara:osdi08,wu:loom:osdi10,cfix:osdi12}. 
On one hand, these techniques can be deployed in \xxx's backups and help 
\xxx detect data races. On the other hand, \xxx's asynchronous replication 
architecture can mitigate the performance overhead of these powerful 
analyses~\cite{repframe:apsys15}.


