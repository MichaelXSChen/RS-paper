\section{Related Work} \label{sec:related}


\para{Software-based consensus.} There are a rich set of
\paxos algorithms~\cite{paxos:practical,paxos,paxos:simple,paxos:complex,
epaxos:sosp13} and 
implementations~\cite{paxos:live,paxos:practical,chubby:osdi,crane:sosp15}. 
\paxos is notoriously difficult to be fast and 
scalable~\cite{ellis:thesis,manos:hotdep10,scatter:sosp11}. Since consensus 
protocols play a core role in datacenters~\cite{matei:hotcloud11, mesos:nsdi11, 
datacenter:os} and worldwide 
distributed systems~\cite{spanner:osdi12,mencius:osdi08}, a variety of study 
have been conducted to improve specific aspects of consensus protocols, 
including order commutativity~\cite{epaxos:sosp13}, 
understandability~\cite{raft:usenix14,paxos}, and verifiable reliability 
rules~\cite{modist:nsdi09,demeter:sosp11}.

% Although \xxx tightly integrates 
% RDMA features in \paxos, its implementation mostly complies with a popular, 
% practical approach~\cite{paxos:practical} for reliability. Other \paxos 
% approaches can also be leveraged in \xxx.


To make \paxos's throughput scalable (\ie, more replicas, higher throughput), 
various systems leverage \paxos as a core building block to develop advanced 
replication approaches, including partitioning program 
states~\cite{scatter:sosp11,ssmr:dsn14}, splitting consensus 
leadership~\cite{mencius:osdi08,spaxos:srds12}, and hierarchical 
replication~\cite{manos:hotdep10,scatter:sosp11}. Theses approaches have shown 
to largely improve throughput. However, the core of these systems, 
\paxos, still faces an unscalable consensus 
latency~\cite{ellis:thesis,scatter:sosp11,manos:hotdep10}. By using \xxx as a 
building block, these system may scale even better.

% arious advanced replication models have been 
% developed~\cite{epaxos:sosp13,mencius:osdi08,scatter:sosp11,manos:hotdep10}. 



Three state machine replication (SMR) systems, Eve~\cite{eve:osdi12}, 
Rex~\cite{rex:eurosys14}, and Crane~\cite{crane:sosp15}, build traditional 
\paxos protocols to improve the availability of general server programs. 
Evaluation in these systems shows that SMR services incur modest overhead on 
server programs' throughput compared to their unreplicated executions. \xxx's 
consensus latency is much faster than their traditional 
protocols (\eg, see \crane in Figure~\ref{fig:scalability}).
% If programs (\eg, key-value 
% stores) demand short request latency, \xxx  
% \xxx's (\S\ref{sec:compare}).
% These systems can leverage \xxx's general, 
% RDMA-accelerated protocol to improve latency.

% % DARE: simply use RDMA writes to do consensus on the leader side.
% To further improve consensus speed, DARE~\cite{dare:hpdc15} proposes a second 
% approach by simply replacing message passing in \paxos with one-sided RDMA 
% operations. For speed, DARE lets the leader handle a whole consensus round
% with three steps. The leader first appends a consensus request to a consensus 
% log in all remote replicas with RDMA writes. For the the successful writes with 
% ACKs, it then updates the tail pointers in remote logs and wait ACKs of these 
% updates. Finally, the leader knows that the minimum tail pointer among at 
% least a majority of replcias reach consensus.

% Two systems Rex and Eve did not provide latency evaluation; not open source.
% Two systems Calvin and Crane, we ran them with Calvin's database; we are XX 
% times faster.

% Notably, Eve~\cite{eve:osdi12} presents an execution state checking approach 
% based on their \paxos coordination service. Eve's completeness on detecting 
% execution divergence relies on whether developers have manually annotated all 
% thread-shared states in program code. \xxx's output checking approach is 
% automated (no manual code annotation is needed), and its completeness depends on 
% whether the diverged execution states propagate to network outputs. Eve and 
% \xxx's checking approaches are complementary and can be integrated.

% Two DARE limitations: first, no stable storage. second, old leader may 
% % unsafely writes to remote logs and thought it has reached a consensus.
% The second category includes DARE~\cite{dare:hpdc15}, a coordination protocol 
% that also uses RDMA to reduce latency. Part of \xxx's implementation was 
% inspired by DARE. \xxx differs from DARE in two major aspects.
% 
% The first difference is in a reliability model level. DARE's model is different 
% from standard \paxos's: DARE assumes that a replica's memory is still accessible 
% to remote replicas even if this replica's CPU fails, so that DARE's leader can 
% still write to remote backups. With this reliability model, DARE requires four 
% one-sided RDMA writes in each consensus round between the leader and a backup. 
% DARE's paper shows that the MTTF (mean time to failure) of memory and CPU are 
% similar.
% 
% \xxx's reliability model complies with standard \paxos's: memory 
% and CPU may fail, thus consensus requests must be written to stable storage. 
% \xxx requires two one-sided RDMA writes and two SSD writes on a consensus 
% round between the leader and a backup. DARE reported a $\sim$15 \us consensus 
% latency for write requests and $\sim$7 \us for read requests on a 64-byte 
% payload. \xxx's consensus latency is compatible with DARE's on a similar 
% payload size (\eg, \xxx's consensus latency for \ssdb was 14.6 \us in 
% Table~\ref{tab:consensus-latency}).

% Second, in a protocol level, DARE does pure-leader consensus in normal case 
% for efficiency. The leader writes a consensus request to remote replicas' 
% logs. If a majority of writes succeed with RDMA ACKs, the leader thinks that a 
% consensus has reached. DARE's backups do not invoke any proposal 
% number~\cite{paxos:simple} or view~\cite{paxos:practical} checking during 
% consensus.
% 
% In general, we argue that a pure-leader consensus approach is difficult to 
% guarantee safety. For instance, a leader may disconnect from the network 
% shortly (\eg, a temporary NIC error), reconnect, and then invoke 
% consensus requests. Concurrently, backups may start a leader election. It could 
% be risky that the leader's RDMA writes succeed and it ``thinks" that a 
% consensus has reached (because it handles everything of a consensus without any 
% backup involvement), but backups may have inconsistently started a leader 
% election.
% 
% Even if a backup first carefully does a check on a heartbeat timeout (or a 
% latest consensus request), it then closes the RDMA QP with this ``leader", 
% these two operations still have a vulnerable time window and the ``leader" can 
% write in between. Allowing a leader to ``think" that it has got a consensus 
% could be unsafe. This is probably why standard \paxos 
% protocols~\cite{paxos,paxos:simple,paxos:practical} must let backups do proposal 
% number of view checking rather than taking a pure-leader approach. \xxx complies 
% with standard \paxos protocol~\cite{paxos:practical}: a \xxx backup always 
% first compares its own view ID with the view ID in a consensus request, it then 
% agrees or rejects this request.

% The second difference is in an application level. DARE's evaluation used a 
% 335-line, single-threaded key-value store. \xxx aims to support general 
% programs.

% Various paxos protocols. Ours is complementary. Can be plugged into our 
% system.

% Three systems aim to provide SMR services to server programs. Eve, Rex, Dare, 
% Crane.
% Type I: Dare. Weaker durability. Four RDMA writes. We have two RDMA writes 
% plus two SSD writes. Compatible perf. Not for general server programs.

% Type II: Eve, Rex, Crane. All slow. Eve has output check, but also slow, use 
% TCP, and needs annotation.


\para{Hardware- or Network- assisted consensus.}  Three recent
systems~\cite{specpaxos:nsdi15,nopaxos:osdi16,consensusbox:nsdi16} leverage 
augmented network hardware or topology to improve \paxos consensus latency. 
``Consensus in a Box"~\cite{consensusbox:nsdi16} implemented \zookeeper's 
consensus protocol in FPGA and intensively modify its hardware TCP/IP stack for 
fast latency. It implemented a key-value store in hardware and showed similar 
consensus latency to \xxx on 3 or 5 replicas. It did not discuss or evaluate 
its scalability. Compared to this protocol, \xxx does not need to modify 
hardware, so it is easier to deploy and support general programs.

Speculative Paxos~\cite{specpaxos:nsdi15} and NOPaxos~\cite{nopaxos:osdi16} 
leverage the synchrony feature of datacenter topology to order requests, so 
they can eliminate consensus rounds if packets are not reordered or lost. 
Moreover, NOPaxos assigns a global sequence ID to packets so that it can 
proactively detect reordered or lost packets. If packets are lost or 
reordered, they invoke consensus to rescue. These two systems are not designed 
for scalability because when replica group and the datacenter network 
become big, the probability that replicas incur reordered or lost packets will 
increase. Moreover, these two systems' consensus modules go through TCP/IP 
layers and incur high round-trip latency. \xxx can help them.

\para{RDMA-based systems.} RDMA techniques have been implemented in various 
architectures, including Infiniband~\cite{infiniband}, RoCE~\cite{roce}, and 
iWRAP~\cite{iwrap}. RDMA have been leveraged in many systems to improve 
application-specific latency and throughput, including high performance 
computing~\cite{openmpi}, key-value 
stores~\cite{pilaf:usenix14,herd:sigcomm14,farm:nsdi14,memcached:rdma}, 
transactional processing systems~\cite{drtm:sosp15,farm:sosp15}, and file 
systems~\cite{gibson:nfs}. For instance, FaRM~\cite{farm:nsdi14} leverages RDMA 
to build a fast DHT. FaRM uses a data replication 
approach~\cite{ramcloud:sosp11}, which works in a primary-backup 
manner~\cite{remus:nsdi08}. In general, \paxos provides better consistency and 
availability than primary-backup. These RDMA-based systems use RDMA to improve 
performance in different aspects, so they are complementary to \xxx.
% It will be interesting to investigate whether \xxx can improve the 
% availability for both the client and server for some of these advanced systems 
% within a datacenter, and we leave it for future work.

% \xxx's deployment model is to 
% provide SMR fault-tolerance to general server programs deployed in datacenters, 
% and the client programs access these server programs in LAN or WAN. 

% We didn't evaluate these systems because they are not available on client sides.


% Increasing assurance of replicas on thread schedulings. Not general 
% as Falcon's checkpoint techniques. But complementary for example ClamAV.
% \para{Nondeterminism.} 
% Nondeterminism~\cite{racepro:sosp11,dmp:asplos09,coredet:asplos10,
% cui:tern:osdi10, kendo:asplos09,
% dthreads:sosp11,peregrine:sosp11,parrot:sosp13,determinator:osdi10} is 
% pervasive in both application programs and OS kernels, and it often comes with 
% concurrency bugs~\cite{lu:concurrency-bugs}. To mitigate nondeterminism, 
% deterministic multithreading techniques~\cite{grace:oopsla09, 
% dthreads:sosp11, 
% determinator:osdi10,dpj:oopsla09, 
% dmp:asplos09,kendo:asplos09,coredet:asplos10,dos:osdi10,ddos:asplos13,
% ics:oopsla13} and deterministic replay 
% techniques~\cite{r2:osdi,friday2007,srinivasan:flashback,revirt,dejavu,
% vmware-record-replay,smp-revirt:vee08,pres:sosp09,scribe:sigmetrics10,
% odr:sosp09, capo:asplos09} have been developed. Much of these techniques can 
% greatly improve software reliability, but they often come with a performance 
% slowdown. \xxx can run these techniques with the server program to mitigate 
% replica divergence caused by concurrency bugs. 

% \para{Concurrency.} Pervasive in real-world applications. Threads. Processes. 
% Break SMR's state machine assumption. complementary.
% \para{Concurrency.} \xxx are mutually beneficial with much prior work on 
% concurrency error 
% detection~\cite{yu:racetrack:sosp,savage:eraser,racerx:sosp03,lu:muvi:sosp,
% avio:asplos06,conmem:asplos10},
% diagnosis~\cite{racefuzzer:pldi08,ctrigger:asplos09,atomfuzzer:fse08}, and
% correction~\cite{dimmunix:osdi08,gadara:osdi08,wu:loom:osdi10,cfix:osdi12}. 
% On one hand, these techniques can be deployed in \xxx's backups and help 
% \xxx detect data races. On the other hand, \xxx's asynchronous replication 
% architecture can mitigate the performance overhead of these powerful 
% analyses~\cite{repframe:apsys15}.


