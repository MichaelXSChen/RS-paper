\section{Related Work} \label{sec:related}


\para{Software-based consensus.} SMR is a powerful, but 
complex fault-tolerance technique. The literature has developeed a rich set of
\paxos 
algorithms~\cite{paxos:practical,paxos,paxos:simple,paxos:complex,epaxos:sosp13}
and implementations~\cite{paxos:live,paxos:practical,chubby:osdi}. \paxos is 
notoriously difficult to be fast and scalable~\cite{ellis:thesis}. To improve 
speed and scalability, various advanced replication models have been 
developed~\cite{epaxos:sosp13,mencius:osdi08,scatter:sosp11,manos:hotdep10}. 
Since consensus protocols play a core role in 
datacenters~\cite{matei:hotcloud11, mesos:nsdi11, datacenter:os} and distributed 
systems~\cite{spanner:osdi12,mencius:osdi08}, a variety of study have been 
conducted to improve different aspects of consensus protocols, including 
performance~\cite{epaxos:sosp13,paxos:fast,dare:hpdc15}, 
understandability~\cite{raft:usenix14,paxos}, and verifiable reliability 
rules~\cite{modist:nsdi09,demeter:sosp11}. Although \xxx tightly integrates 
RDMA features in \paxos, its implementation mostly complies with a popular, 
practical approach~\cite{paxos:practical} for reliability. Other \paxos 
approaches can also be leveraged in \xxx.

Five systems aim to provide SMR or similar fault-tolerance guarantees to server 
programs and thus they are the most relevant to \xxx. They can be divided into 
two categories depending on whether their protocols run on TCP/IP or RDMA. 
The first category runs on TCP/IP, including Eve~\cite{eve:osdi12}, 
Rex~\cite{rex:eurosys14}, Calvin~\cite{calvin:sigmod12}, and 
Crane~\cite{crane:sosp15}. Evaluation in these systems shows that SMR 
services incur modest overhead on server programs' throughput compared to their 
unreplicated executions. However, for some other programs (\eg, key-value 
stores) demanding a short response time, \xxx is more suitable because these 
systems' consensus latency is at least 10X slower than 
\xxx's (\S\ref{sec:compare}).
% These systems can leverage \xxx's general, 
% RDMA-accelerated protocol to improve latency.

% % DARE: simply use RDMA writes to do consensus on the leader side.
% To further improve consensus speed, DARE~\cite{dare:hpdc15} proposes a second 
% approach by simply replacing message passing in \paxos with one-sided RDMA 
% operations. For speed, DARE lets the leader handle a whole consensus round
% with three steps. The leader first appends a consensus request to a consensus 
% log in all remote replicas with RDMA writes. For the the successful writes with 
% ACKs, it then updates the tail pointers in remote logs and wait ACKs of these 
% updates. Finally, the leader knows that the minimum tail pointer among at 
% least a majority of replcias reach consensus.

% Two systems Rex and Eve did not provide latency evaluation; not open source.
% Two systems Calvin and Crane, we ran them with Calvin's database; we are XX 
% times faster.

Notably, Eve~\cite{eve:osdi12} presents an execution state checking approach 
based on their \paxos coordination service. Eve's completeness on detecting 
execution divergence relies on whether developers have manually annotated all 
thread-shared states in program code. \xxx's output checking approach is 
automated (no manual code annotation is needed), and its completeness depends on 
whether the diverged execution states propagate to network outputs. Eve and 
\xxx's checking approaches are complementary and can be integrated.

% Two DARE limitations: first, no stable storage. second, old leader may 
% % unsafely writes to remote logs and thought it has reached a consensus.
% The second category includes DARE~\cite{dare:hpdc15}, a coordination protocol 
% that also uses RDMA to reduce latency. Part of \xxx's implementation was 
% inspired by DARE. \xxx differs from DARE in two major aspects.
% 
% The first difference is in a reliability model level. DARE's model is different 
% from standard \paxos's: DARE assumes that a replica's memory is still accessible 
% to remote replicas even if this replica's CPU fails, so that DARE's leader can 
% still write to remote backups. With this reliability model, DARE requires four 
% one-sided RDMA writes in each consensus round between the leader and a backup. 
% DARE's paper shows that the MTTF (mean time to failure) of memory and CPU are 
% similar.
% 
% \xxx's reliability model complies with standard \paxos's: memory 
% and CPU may fail, thus consensus requests must be written to stable storage. 
% \xxx requires two one-sided RDMA writes and two SSD writes on a consensus 
% round between the leader and a backup. DARE reported a $\sim$15 \us consensus 
% latency for write requests and $\sim$7 \us for read requests on a 64-byte 
% payload. \xxx's consensus latency is compatible with DARE's on a similar 
% payload size (\eg, \xxx's consensus latency for \ssdb was 14.6 \us in 
% Table~\ref{tab:consensus-latency}).

% Second, in a protocol level, DARE does pure-leader consensus in normal case 
% for efficiency. The leader writes a consensus request to remote replicas' 
% logs. If a majority of writes succeed with RDMA ACKs, the leader thinks that a 
% consensus has reached. DARE's backups do not invoke any proposal 
% number~\cite{paxos:simple} or view~\cite{paxos:practical} checking during 
% consensus.
% 
% In general, we argue that a pure-leader consensus approach is difficult to 
% guarantee safety. For instance, a leader may disconnect from the network 
% shortly (\eg, a temporary NIC error), reconnect, and then invoke 
% consensus requests. Concurrently, backups may start a leader election. It could 
% be risky that the leader's RDMA writes succeed and it ``thinks" that a 
% consensus has reached (because it handles everything of a consensus without any 
% backup involvement), but backups may have inconsistently started a leader 
% election.
% 
% Even if a backup first carefully does a check on a heartbeat timeout (or a 
% latest consensus request), it then closes the RDMA QP with this ``leader", 
% these two operations still have a vulnerable time window and the ``leader" can 
% write in between. Allowing a leader to ``think" that it has got a consensus 
% could be unsafe. This is probably why standard \paxos 
% protocols~\cite{paxos,paxos:simple,paxos:practical} must let backups do proposal 
% number of view checking rather than taking a pure-leader approach. \xxx complies 
% with standard \paxos protocol~\cite{paxos:practical}: a \xxx backup always 
% first compares its own view ID with the view ID in a consensus request, it then 
% agrees or rejects this request.

% The second difference is in an application level. DARE's evaluation used a 
% 335-line, single-threaded key-value store. \xxx aims to support general 
% programs.

% Various paxos protocols. Ours is complementary. Can be plugged into our 
% system.

% Three systems aim to provide SMR services to server programs. Eve, Rex, Dare, 
% Crane.
% Type I: Dare. Weaker durability. Four RDMA writes. We have two RDMA writes 
% plus two SSD writes. Compatible perf. Not for general server programs.

% Type II: Eve, Rex, Crane. All slow. Eve has output check, but also slow, use 
% TCP, and needs annotation.


\para{Hardware- or Network- assisted consensus.} 

\para{RDMA-based techniques.} RDMA techniques have been implemented in various 
architectures, including Infiniband~\cite{infiniband}, RoCE~\cite{roce}, and 
iWRAP~\cite{iwrap}. RDMA have been leveraged in many systems to improve 
application-specific latency and throughput, including high performance 
computing~\cite{openmpi}, key-value 
stores~\cite{pilaf:usenix14,herd:sigcomm14,farm:nsdi14,memcached:rdma}, 
transactional processing systems~\cite{drtm:sosp15,farm:sosp15}, and file 
systems~\cite{gibson:nfs}. These systems are largely complementary to \xxx. It 
will be interesting to investigate whether \xxx can improve the availability for 
both the client and server for some of these advanced systems within a 
datacenter, and we leave it for future work.

% \xxx's deployment model is to 
% provide SMR fault-tolerance to general server programs deployed in datacenters, 
% and the client programs access these server programs in LAN or WAN. 

% We didn't evaluate these systems because they are not available on client sides.


% Increasing assurance of replicas on thread schedulings. Not general 
% as Falcon's checkpoint techniques. But complementary for example ClamAV.
% \para{Nondeterminism.} 
% Nondeterminism~\cite{racepro:sosp11,dmp:asplos09,coredet:asplos10,
% cui:tern:osdi10, kendo:asplos09,
% dthreads:sosp11,peregrine:sosp11,parrot:sosp13,determinator:osdi10} is 
% pervasive in both application programs and OS kernels, and it often comes with 
% concurrency bugs~\cite{lu:concurrency-bugs}. To mitigate nondeterminism, 
% deterministic multithreading techniques~\cite{grace:oopsla09, 
% dthreads:sosp11, 
% determinator:osdi10,dpj:oopsla09, 
% dmp:asplos09,kendo:asplos09,coredet:asplos10,dos:osdi10,ddos:asplos13,
% ics:oopsla13} and deterministic replay 
% techniques~\cite{r2:osdi,friday2007,srinivasan:flashback,revirt,dejavu,
% vmware-record-replay,smp-revirt:vee08,pres:sosp09,scribe:sigmetrics10,
% odr:sosp09, capo:asplos09} have been developed. Much of these techniques can 
% greatly improve software reliability, but they often come with a performance 
% slowdown. \xxx can run these techniques with the server program to mitigate 
% replica divergence caused by concurrency bugs. 

% \para{Concurrency.} Pervasive in real-world applications. Threads. Processes. 
% Break SMR's state machine assumption. complementary.
% \para{Concurrency.} \xxx are mutually beneficial with much prior work on 
% concurrency error 
% detection~\cite{yu:racetrack:sosp,savage:eraser,racerx:sosp03,lu:muvi:sosp,
% avio:asplos06,conmem:asplos10},
% diagnosis~\cite{racefuzzer:pldi08,ctrigger:asplos09,atomfuzzer:fse08}, and
% correction~\cite{dimmunix:osdi08,gadara:osdi08,wu:loom:osdi10,cfix:osdi12}. 
% On one hand, these techniques can be deployed in \xxx's backups and help 
% \xxx detect data races. On the other hand, \xxx's asynchronous replication 
% architecture can mitigate the performance overhead of these powerful 
% analyses~\cite{repframe:apsys15}.


