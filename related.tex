\section{Related Work} \label{sec:related}


\para{State machine replication.} \para{State machine replication (SMR).}  SMR 
has been studied by the literature 
for decades, and it is recognized by both industry and academia as a powerful 
fault-tolerance technique in clouds and distributed 
systems~\cite{lamportclock,smr:tutorial}. As a common practice, SMR uses 
\paxos~\cite{paxos,paxos:simple,paxos:complex} and its popular engineering 
approaches~\cite{paxos:live,paxos:practical} as the consensus protocol to 
ensure that all replicas see the same input request sequence. Since consensus 
protocols are the core of SMR, a variety of study improve different aspects of 
consensus protocols, including performance~\cite{epaxos:sosp13,paxos:fast} and 
understandability~\cite{raft:usenix14}. Although \xxx's current implementation 
takes a popular engineering approach~\cite{paxos:practical} for practicality, 
it can also leverage other consensus protocols and approaches.

Five systems aim to provide SMR or similar services to server programs and 
thus they are the most relevant to \xxx. They can be divided into two 
categories depending on whether they use RDMA to accelerate their services. 
Tet first category includes Eve, Rex, Calvin, and Crane, which 
leverage traditional \paxos protocols (or similar synchronized replication 
protocols, \eg, Zookeeper in Calvin) running on TCP/IP as the coordination 
service. The evaluation in these systems have shown that SMR services incur 
modest overhead on server programs' throughput compared to their unreplicated 
executions. However, for key-value stores that are extremely critical on 
latency, their consensus latency one order of magnitude higher than that in 
\xxx (\S\ref{sec:compare}). These systems can leverage \xxx's general, 
RDMA-accelerated protocol to improve latency.

% % DARE: simply use RDMA writes to do consensus on the leader side.
% To further improve consensus speed, DARE~\cite{dare:hpdc15} proposes a second 
% approach by simply replacing message passing in \paxos with one-sided RDMA 
% operations. For speed, DARE lets the leader handle a whole consensus round
% with three steps. The leader first appends a consensus request to a consensus 
% log in all remote replicas with RDMA writes. For the the successful writes with 
% ACKs, it then updates the tail pointers in remote logs and wait ACKs of these 
% updates. Finally, the leader knows that the minimum tail pointer among at 
% least a majority of replcias reach consensus.

% Two systems Rex and Eve did not provide latency evaluation; not open source.
% Two systems Calvin and Crane, we ran them with Calvin's database; we are XX 
% times faster.

Notably, Eve presents a execution state checking approach based on their 
coordination service. Eve's completeness on detecting execution divergence 
relies on whether developers have manually annotated all thread-shared states 
in program code. \xxx's output checking approach is automated (no manuall code 
annotation is needed), and its completeness depends on whether the diverged 
execution states propogate to network outputs. These two approaches are 
complementary and can be integrated.

% Two DARE limitations: first, no stable storage. second, old leader may 
% % unsafely writes to remote logs and thought it has reached a consensus.
The second category includes DARE, a coordination protocol that also uses RDMA 
to reduce latency. \xxx differs from DARE in two aspects. First, 
the reliability model in DARE is different from that in \paxos: DARE assumes 
that a replica's memory is still accessible to remote replicas even if CPU 
fails, so that the leader still writes to remote backups. With this reliability 
model, DARE requires four one-sided RDMA writes on a consensus round between the 
leader and a backup. DARE's paper shows that the MTTF (mean time to failure) of 
memory and CPU is similar. In contrast, \xxx's reliability model aligns with 
\paxos's: memory and CPU may fail, thus consensus requests must be written to 
stable storage. Thus, \xxx requires two one-sided RDMA writes and two SSD writes 
on a consensus round between the leader and a backup. Our evaluation shows that 
\xxx's latency is compatible with DARE's. The second difference between DARE 
and \xxx is that \xxx aims to support general, diverse server programs, while 
DARE's evaluation uses a XX-line key-value store.

% Various paxos protocols. Ours is complementary. Can be plugged into our 
% system.

% Three systems aim to provide SMR services to server programs. Eve, Rex, Dare, 
% Crane.
% Type I: Dare. Weaker durability. Four RDMA writes. We have two RDMA writes 
% plus two SSD writes. Compatible perf. Not for general server programs.

% Type II: Eve, Rex, Crane. All slow. Eve has output check, but also slow, use 
% TCP, and needs annotation.


\para{RDMA techniques.} RDMA techniques have been leveraged in many online 
storage systems to improve latency and throughput within datacenters. These 
systems include key-value stores~\cite{jinyang,herd,farm}, transactional 
processing systems~\cite{haibo,microsoft:sosp15}, and XXX. These systems mainly 
aim to use RDMA to speedup specific communications, where both their storage 
and client access have RDMA enabled. \xxx's deployment model is to provide 
SMR fault-tolerance to general server programs deployed in datacenters, and the 
client programs access these server programs in LAN or WAN. It 
would be interesting to investigate whether \xxx can improve the availability 
for both the client side and server side of these advanced systems within a 
datacenter, and we leave it for future work.

% We didn't evaluate these systems because they are not available on client sides.


% Increasing assurance of replicas on thread schedulings. Not general 
% as Falcon's checkpoint techniques. But complementary for example ClamAV.
\para{Determinism techniques.} In order to make multi-threading easier to 
understand, test, analyze, and replicate, researchers have built two types of 
reliable multi-threading systems: (1) stable multi-threading systems (or 
\smt)~\cite{grace:oopsla09, dthreads:sosp11, determinator:osdi10} that aim to 
reduce the number of possible thread interleavings for program all inputs, and 
(2) deterministic multi-threading systems (or \dmt)~\cite{dpj:oopsla09, 
dmp:asplos09,kendo:asplos09,coredet:asplos10,dos:osdi10,ddos:asplos13,
ics:oopsla13} that aim to reduce the number of possible thread interleavings on 
each program input. Typically, these systems use deterministic logical clocks 
instead of nondeterministic physical clocks to make sure inter-thread 
communications (\eg, \mutexlock and accesses to global variables) can only 
happen at some specific logical clocks. Therefore, given the same or similar 
inputs, these systems can enforce the same thread interleavings and eventually 
the same executions. These systems 
have shown to greatly improve software reliability, including coverage of 
testing inputs~\cite{ics:oopsla13} and speed of recording 
executions\cite{dos:osdi10} for debugging.

% \para{Concurrency.} Pervasive in real-world applications. Threads. Processes. 
% Break SMR's state machine assumption. complementary.
\para{Concurrency.} \xxx are mutually beneficial with much prior work on 
concurrency error 
detection~\cite{yu:racetrack:sosp,savage:eraser,racerx:sosp03,lu:muvi:sosp,
avio:asplos06,conmem:asplos10},
diagnosis~\cite{racefuzzer:pldi08,ctrigger:asplos09,atomfuzzer:fse08}, and
correction~\cite{dimmunix:osdi08,gadara:osdi08,wu:loom:osdi10,cfix:osdi12}. 
On one hand, these techniques can be deployed in \xxx's backups and help 
\xxx detect data races. On the other hand, \xxx's asynchronous replication 
architecture can mitigate the performance overhead of these powerful 
analyses~\cite{repframe:apsys15}.


